{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transition_check.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhzCcb1s6vVJXF4A5c1JuH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/stable-baselines/blob/main/Transition_check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZcLMxoe8mWs",
        "outputId": "6baa3807-72d5-43c8-9b53-83bebac33837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfdqjsZl9a4k"
      },
      "source": [
        "!apt install swig\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Qz13G89q4U"
      },
      "source": [
        "CartPoleEnv Class에 step2 추가\n",
        "- action만을 외부 입력으로 받는 step함수와 달리\n",
        "- state와 action을 외부 입력으로 받아서 고정된 s_t, a_t에 따라 s_t+1이 어떻게 변화는 지 확인하려 함\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdDcJy0D9cuW"
      },
      "source": [
        "\"\"\"\n",
        "Classic cart-pole system implemented by Rich Sutton et al.\n",
        "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
        "permalink: https://perma.cc/C9ZM-652R\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import gym\n",
        "from gym import spaces, logger\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CartPoleEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
        "        a frictionless track. The pendulum starts upright, and the goal is to\n",
        "        prevent it from falling over by increasing and reducing the cart's\n",
        "        velocity.\n",
        "\n",
        "    Source:\n",
        "        This environment corresponds to the version of the cart-pole problem\n",
        "        described by Barto, Sutton, and Anderson\n",
        "\n",
        "    Observation:\n",
        "        Type: Box(4)\n",
        "        Num     Observation               Min                     Max\n",
        "        0       Cart Position             -4.8                    4.8\n",
        "        1       Cart Velocity             -Inf                    Inf\n",
        "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
        "        3       Pole Angular Velocity     -Inf                    Inf\n",
        "\n",
        "    Actions:\n",
        "        Type: Discrete(2)\n",
        "        Num   Action\n",
        "        0     Push cart to the left\n",
        "        1     Push cart to the right\n",
        "\n",
        "        Note: The amount the velocity that is reduced or increased is not\n",
        "        fixed; it depends on the angle the pole is pointing. This is because\n",
        "        the center of gravity of the pole increases the amount of energy needed\n",
        "        to move the cart underneath it\n",
        "\n",
        "    Reward:\n",
        "        Reward is 1 for every step taken, including the termination step\n",
        "\n",
        "    Starting State:\n",
        "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
        "\n",
        "    Episode Termination:\n",
        "        Pole Angle is more than 12 degrees.\n",
        "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
        "        the display).\n",
        "        Episode length is greater than 200.\n",
        "        Solved Requirements:\n",
        "        Considered solved when the average return is greater than or equal to\n",
        "        195.0 over 100 consecutive trials.\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\n",
        "        'render.modes': ['human', 'rgb_array'],\n",
        "        'video.frames_per_second': 50\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        self.gravity = 9.8\n",
        "        self.masscart = 1.0\n",
        "        self.masspole = 0.1\n",
        "        self.total_mass = (self.masspole + self.masscart)\n",
        "        self.length = 0.5  # actually half the pole's length\n",
        "        self.polemass_length = (self.masspole * self.length)\n",
        "        self.force_mag = 10.0\n",
        "        self.tau = 0.02  # seconds between state updates\n",
        "        self.kinematics_integrator = 'euler'\n",
        "\n",
        "        # Angle at which to fail the episode\n",
        "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
        "        self.x_threshold = 2.4\n",
        "\n",
        "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
        "        # is still within bounds.\n",
        "        high = np.array([self.x_threshold * 2,\n",
        "                         np.finfo(np.float32).max,\n",
        "                         self.theta_threshold_radians * 2,\n",
        "                         np.finfo(np.float32).max],\n",
        "                        dtype=np.float32)\n",
        "\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
        "\n",
        "        self.seed()\n",
        "        self.viewer = None\n",
        "        self.state = None\n",
        "\n",
        "        self.steps_beyond_done = None\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, action):\n",
        "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
        "        assert self.action_space.contains(action), err_msg\n",
        "\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        force = self.force_mag if action == 1 else -self.force_mag\n",
        "        costheta = math.cos(theta)\n",
        "        sintheta = math.sin(theta)\n",
        "\n",
        "        # For the interested reader:\n",
        "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
        "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
        "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
        "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
        "\n",
        "        if self.kinematics_integrator == 'euler':\n",
        "            x = x + self.tau * x_dot\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "        else:  # semi-implicit euler\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            x = x + self.tau * x_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "\n",
        "        self.state = (x, x_dot, theta, theta_dot)\n",
        "\n",
        "        done = bool(\n",
        "            x < -self.x_threshold\n",
        "            or x > self.x_threshold\n",
        "            or theta < -self.theta_threshold_radians\n",
        "            or theta > self.theta_threshold_radians\n",
        "        )\n",
        "\n",
        "        if not done:\n",
        "            reward = 1.0\n",
        "        elif self.steps_beyond_done is None:\n",
        "            # Pole just fell!\n",
        "            self.steps_beyond_done = 0\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            if self.steps_beyond_done == 0:\n",
        "                logger.warn(\n",
        "                    \"You are calling 'step()' even though this \"\n",
        "                    \"environment has already returned done = True. You \"\n",
        "                    \"should always call 'reset()' once you receive 'done = \"\n",
        "                    \"True' -- any further steps are undefined behavior.\"\n",
        "                )\n",
        "            self.steps_beyond_done += 1\n",
        "            reward = 0.0\n",
        "\n",
        "        return np.array(self.state), reward, done, {}\n",
        "        \n",
        "    ##### step2 추가 #######\n",
        "    def step2(self, state, action):\n",
        "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
        "        assert self.action_space.contains(action), err_msg\n",
        "\n",
        "        x, x_dot, theta, theta_dot = state # self.state\n",
        "        force = self.force_mag if action == 1 else -self.force_mag\n",
        "        costheta = math.cos(theta)\n",
        "        sintheta = math.sin(theta)\n",
        "\n",
        "        # For the interested reader:\n",
        "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
        "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
        "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
        "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
        "\n",
        "        if self.kinematics_integrator == 'euler':\n",
        "            x = x + self.tau * x_dot\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "        else:  # semi-implicit euler\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            x = x + self.tau * x_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "\n",
        "        self.state = (x, x_dot, theta, theta_dot)\n",
        "\n",
        "        done = bool(\n",
        "            x < -self.x_threshold\n",
        "            or x > self.x_threshold\n",
        "            or theta < -self.theta_threshold_radians\n",
        "            or theta > self.theta_threshold_radians\n",
        "        )\n",
        "\n",
        "        if not done:\n",
        "            reward = 1.0\n",
        "        elif self.steps_beyond_done is None:\n",
        "            print ('# Pole just fell!')\n",
        "            print (self.state)\n",
        "            self.steps_beyond_done = 0\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            if self.steps_beyond_done == 0:\n",
        "                logger.warn(\n",
        "                    \"You are calling 'step()' even though this \"\n",
        "                    \"environment has already returned done = True. You \"\n",
        "                    \"should always call 'reset()' once you receive 'done = \"\n",
        "                    \"True' -- any further steps are undefined behavior.\"\n",
        "                )\n",
        "            self.steps_beyond_done += 1\n",
        "            reward = 0.0\n",
        "            print ('# reward = 0')\n",
        "            print (self.state)\n",
        "\n",
        "        return np.array(self.state), reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
        "        self.steps_beyond_done = None\n",
        "        return np.array(self.state)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        screen_width = 600\n",
        "        screen_height = 400\n",
        "\n",
        "        world_width = self.x_threshold * 2\n",
        "        scale = screen_width/world_width\n",
        "        carty = 100  # TOP OF CART\n",
        "        polewidth = 10.0\n",
        "        polelen = scale * (2 * self.length)\n",
        "        cartwidth = 50.0\n",
        "        cartheight = 30.0\n",
        "\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
        "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
        "            axleoffset = cartheight / 4.0\n",
        "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
        "            self.carttrans = rendering.Transform()\n",
        "            cart.add_attr(self.carttrans)\n",
        "            self.viewer.add_geom(cart)\n",
        "            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
        "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
        "            pole.set_color(.8, .6, .4)\n",
        "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
        "            pole.add_attr(self.poletrans)\n",
        "            pole.add_attr(self.carttrans)\n",
        "            self.viewer.add_geom(pole)\n",
        "            self.axle = rendering.make_circle(polewidth/2)\n",
        "            self.axle.add_attr(self.poletrans)\n",
        "            self.axle.add_attr(self.carttrans)\n",
        "            self.axle.set_color(.5, .5, .8)\n",
        "            self.viewer.add_geom(self.axle)\n",
        "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
        "            self.track.set_color(0, 0, 0)\n",
        "            self.viewer.add_geom(self.track)\n",
        "\n",
        "            self._pole_geom = pole\n",
        "\n",
        "        if self.state is None:\n",
        "            return None\n",
        "\n",
        "        # Edit the pole polygon vertex\n",
        "        pole = self._pole_geom\n",
        "        l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
        "        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
        "\n",
        "        x = self.state\n",
        "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
        "        self.carttrans.set_translation(cartx, carty)\n",
        "        self.poletrans.set_rotation(-x[2])\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
        "\n",
        "    def close(self):\n",
        "        if self.viewer:\n",
        "            self.viewer.close()\n",
        "            self.viewer = None"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlS4XfIf_Ncm"
      },
      "source": [
        "CartPoleEnv_source Class에도 step2 추가\n",
        "- force_mag = 1000으로 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxAISg829o8W"
      },
      "source": [
        "#from gym.envs.classic_control import cartpole\n",
        "\n",
        "#class CartPoleEnv_source(cartpole.CartPoleEnv):\n",
        "class CartPoleEnv_source(CartPoleEnv):\n",
        "    def __init__(self):\n",
        "        super(CartPoleEnv_source, self).__init__()\n",
        "        self.max_episode_steps = 500 # v3\n",
        "        self._elapsed_steps = 0\n",
        "        # self.gravity = 1.0\n",
        "        self.force_mag = 1000.0\n",
        "        # self.polemass_length = self.polemass_length * 0.5\n",
        "\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = super(CartPoleEnv_source, self).step(action)\n",
        "        self._elapsed_steps += 1\n",
        "        if self._elapsed_steps >= self.max_episode_steps:\n",
        "            info['TimeLimit.truncated'] = not done\n",
        "            done = True\n",
        "        return observation, reward, done, info\n",
        "        \n",
        "    #### step2 추가 #######\n",
        "    def step2(self, state, action):\n",
        "        observation, reward, done, info = super(CartPoleEnv_source, self).step2(state, action)\n",
        "        self._elapsed_steps += 1\n",
        "        if self._elapsed_steps >= self.max_episode_steps:\n",
        "            info['TimeLimit.truncated'] = not done\n",
        "            done = True\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._elapsed_steps = 0\n",
        "        return super(CartPoleEnv_source, self).reset()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmmjoWnENdEr"
      },
      "source": [
        "1차 실험 - gym 환경"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt0tjz2NNhfa"
      },
      "source": [
        "source = CartPoleEnv_source()\n",
        "target = CartPoleEnv()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTH0KfgYPkKf"
      },
      "source": [
        "obs = [0.1,0.1,0.1,0.1]\n",
        "action = 0"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x98YwsM9ONnw",
        "outputId": "6a650ab8-4761-43df-dc8e-df1637b540e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "for i in range(10) :\n",
        "  new_obs, reward, done, info = source.step2(obs, action)\n",
        "  print (new_obs)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0.102      -19.3993984    0.102       29.23232496]\n",
            "[  0.102      -19.3993984    0.102       29.23232496]\n",
            "[  0.102      -19.3993984    0.102       29.23232496]\n",
            "[  0.102      -19.3993984    0.102       29.23232496]\n",
            "[  0.102      -19.3993984    0.102       29.23232496]\n",
            "[  0.102      -19.3993984    0.102       29.23232496]\n",
            "[  0.102      -19.3993984    0.102       29.23232496]\n",
            "[  0.102      -19.3993984    0.102       29.23232496]\n",
            "[  0.102      -19.3993984    0.102       29.23232496]\n",
            "[  0.102      -19.3993984    0.102       29.23232496]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJhxcrhANhb5",
        "outputId": "b7eba45a-552e-4403-d2f5-c9816b2c7400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "for i in range(10) :\n",
        "  new_obs, reward, done, info = target.step2(obs, action)\n",
        "  print (new_obs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.102      -0.09640235  0.102       0.42248276]\n",
            "[ 0.102      -0.09640235  0.102       0.42248276]\n",
            "[ 0.102      -0.09640235  0.102       0.42248276]\n",
            "[ 0.102      -0.09640235  0.102       0.42248276]\n",
            "[ 0.102      -0.09640235  0.102       0.42248276]\n",
            "[ 0.102      -0.09640235  0.102       0.42248276]\n",
            "[ 0.102      -0.09640235  0.102       0.42248276]\n",
            "[ 0.102      -0.09640235  0.102       0.42248276]\n",
            "[ 0.102      -0.09640235  0.102       0.42248276]\n",
            "[ 0.102      -0.09640235  0.102       0.42248276]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htdUl7vSP9b4"
      },
      "source": [
        "2차 실험 - stable-baselines 환경"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbyJsURp_9Xs"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "#from stable_baselines.common import make_vec_env\n",
        "from stable_baselines.common.vec_env import VecNormalize\n",
        "from stable_baselines import A2C\n",
        "from stable_baselines import PPO2\n",
        "from stable_baselines.gail import generate_expert_traj\n",
        "# from darc import darc_envs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukih8x3cETNJ"
      },
      "source": [
        "base_vec_env.py에 step2추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpnT6wloETsw",
        "cellView": "form"
      },
      "source": [
        "#@title base_vec_env.py\n",
        "from abc import ABC, abstractmethod\n",
        "import inspect\n",
        "import pickle\n",
        "from typing import Sequence, Optional, List, Union\n",
        "\n",
        "import cloudpickle\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines import logger\n",
        "from stable_baselines.common.tile_images import tile_images\n",
        "\n",
        "\n",
        "class AlreadySteppingError(Exception):\n",
        "    \"\"\"\n",
        "    Raised when an asynchronous step is running while\n",
        "    step_async() is called again.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        msg = 'already running an async step'\n",
        "        Exception.__init__(self, msg)\n",
        "\n",
        "\n",
        "class NotSteppingError(Exception):\n",
        "    \"\"\"\n",
        "    Raised when an asynchronous step is not running but\n",
        "    step_wait() is called.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        msg = 'not running an async step'\n",
        "        Exception.__init__(self, msg)\n",
        "\n",
        "\n",
        "class VecEnv(ABC):\n",
        "    \"\"\"\n",
        "    An abstract asynchronous, vectorized environment.\n",
        "\n",
        "    :param num_envs: (int) the number of environments\n",
        "    :param observation_space: (Gym Space) the observation space\n",
        "    :param action_space: (Gym Space) the action space\n",
        "    \"\"\"\n",
        "    metadata = {\n",
        "        'render.modes': ['human', 'rgb_array']\n",
        "    }\n",
        "\n",
        "    def __init__(self, num_envs, observation_space, action_space):\n",
        "        self.num_envs = num_envs\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset all the environments and return an array of\n",
        "        observations, or a tuple of observation arrays.\n",
        "\n",
        "        If step_async is still doing work, that work will\n",
        "        be cancelled and step_wait() should not be called\n",
        "        until step_async() is invoked again.\n",
        "\n",
        "        :return: ([int] or [float]) observation\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step_async(self, actions):\n",
        "        \"\"\"\n",
        "        Tell all the environments to start taking a step\n",
        "        with the given actions.\n",
        "        Call step_wait() to get the results of the step.\n",
        "\n",
        "        You should not call this if a step_async run is\n",
        "        already pending.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step2_async(self, state, actions):\n",
        "        \"\"\"\n",
        "        Tell all the environments to start taking a step\n",
        "        with the given actions.\n",
        "        Call step_wait() to get the results of the step.\n",
        "\n",
        "        You should not call this if a step_async run is\n",
        "        already pending.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step_wait(self):\n",
        "        \"\"\"\n",
        "        Wait for the step taken with step_async().\n",
        "\n",
        "        :return: ([int] or [float], [float], [bool], dict) observation, reward, done, information\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step2_wait(self):\n",
        "        \"\"\"\n",
        "        Wait for the step taken with step_async().\n",
        "\n",
        "        :return: ([int] or [float], [float], [bool], dict) observation, reward, done, information\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Clean up the environment's resources.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_attr(self, attr_name, indices=None):\n",
        "        \"\"\"\n",
        "        Return attribute from vectorized environment.\n",
        "\n",
        "        :param attr_name: (str) The name of the attribute whose value to return\n",
        "        :param indices: (list,int) Indices of envs to get attribute from\n",
        "        :return: (list) List of values of 'attr_name' in all environments\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def set_attr(self, attr_name, value, indices=None):\n",
        "        \"\"\"\n",
        "        Set attribute inside vectorized environments.\n",
        "\n",
        "        :param attr_name: (str) The name of attribute to assign new value\n",
        "        :param value: (obj) Value to assign to `attr_name`\n",
        "        :param indices: (list,int) Indices of envs to assign value\n",
        "        :return: (NoneType)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):\n",
        "        \"\"\"\n",
        "        Call instance methods of vectorized environments.\n",
        "\n",
        "        :param method_name: (str) The name of the environment method to invoke.\n",
        "        :param indices: (list,int) Indices of envs whose method to call\n",
        "        :param method_args: (tuple) Any positional arguments to provide in the call\n",
        "        :param method_kwargs: (dict) Any keyword arguments to provide in the call\n",
        "        :return: (list) List of items returned by the environment's method call\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def seed(self, seed: Optional[int] = None) -> List[Union[None, int]]:\n",
        "        \"\"\"\n",
        "        Sets the random seeds for all environments, based on a given seed.\n",
        "        Each individual environment will still get its own seed, by incrementing the given seed.\n",
        "\n",
        "        :param seed: (Optional[int]) The random seed. May be None for completely random seeding.\n",
        "        :return: (List[Union[None, int]]) Returns a list containing the seeds for each individual env.\n",
        "            Note that all list elements may be None, if the env does not return anything when being seeded.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step(self, actions):\n",
        "        \"\"\"\n",
        "        Step the environments with the given action\n",
        "\n",
        "        :param actions: ([int] or [float]) the action\n",
        "        :return: ([int] or [float], [float], [bool], dict) observation, reward, done, information\n",
        "        \"\"\"\n",
        "        self.step_async(actions)\n",
        "        return self.step_wait()\n",
        "\n",
        "    def step2(self, state, actions):\n",
        "        \"\"\"\n",
        "        Step the environments with the given action\n",
        "\n",
        "        :param actions: ([int] or [float]) the action\n",
        "        :return: ([int] or [float], [float], [bool], dict) observation, reward, done, information\n",
        "        \"\"\"\n",
        "        self.step2_async(state, actions)\n",
        "        return self.step2_wait()\n",
        "\n",
        "    def get_images(self) -> Sequence[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Return RGB images from each environment\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def render(self, mode: str = 'human'):\n",
        "        \"\"\"\n",
        "        Gym environment rendering\n",
        "\n",
        "        :param mode: the rendering type\n",
        "        \"\"\"\n",
        "        try:\n",
        "            imgs = self.get_images()\n",
        "        except NotImplementedError:\n",
        "            logger.warn('Render not defined for {}'.format(self))\n",
        "            return\n",
        "\n",
        "        # Create a big image by tiling images from subprocesses\n",
        "        bigimg = tile_images(imgs)\n",
        "        if mode == 'human':\n",
        "            import cv2  # pytype:disable=import-error\n",
        "            cv2.imshow('vecenv', bigimg[:, :, ::-1])\n",
        "            cv2.waitKey(1)\n",
        "        elif mode == 'rgb_array':\n",
        "            return bigimg\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def unwrapped(self):\n",
        "        if isinstance(self, VecEnvWrapper):\n",
        "            return self.venv.unwrapped\n",
        "        else:\n",
        "            return self\n",
        "\n",
        "    def getattr_depth_check(self, name, already_found):\n",
        "        \"\"\"Check if an attribute reference is being hidden in a recursive call to __getattr__\n",
        "\n",
        "        :param name: (str) name of attribute to check for\n",
        "        :param already_found: (bool) whether this attribute has already been found in a wrapper\n",
        "        :return: (str or None) name of module whose attribute is being shadowed, if any.\n",
        "        \"\"\"\n",
        "        if hasattr(self, name) and already_found:\n",
        "            return \"{0}.{1}\".format(type(self).__module__, type(self).__name__)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def _get_indices(self, indices):\n",
        "        \"\"\"\n",
        "        Convert a flexibly-typed reference to environment indices to an implied list of indices.\n",
        "\n",
        "        :param indices: (None,int,Iterable) refers to indices of envs.\n",
        "        :return: (list) the implied list of indices.\n",
        "        \"\"\"\n",
        "        if indices is None:\n",
        "            indices = range(self.num_envs)\n",
        "        elif isinstance(indices, int):\n",
        "            indices = [indices]\n",
        "        return indices\n",
        "\n",
        "\n",
        "class VecEnvWrapper(VecEnv):\n",
        "    \"\"\"\n",
        "    Vectorized environment base class\n",
        "\n",
        "    :param venv: (VecEnv) the vectorized environment to wrap\n",
        "    :param observation_space: (Gym Space) the observation space (can be None to load from venv)\n",
        "    :param action_space: (Gym Space) the action space (can be None to load from venv)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, venv, observation_space=None, action_space=None):\n",
        "        self.venv = venv\n",
        "        VecEnv.__init__(self, num_envs=venv.num_envs, observation_space=observation_space or venv.observation_space,\n",
        "                        action_space=action_space or venv.action_space)\n",
        "        self.class_attributes = dict(inspect.getmembers(self.__class__))\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        self.venv.step_async(actions)\n",
        "\n",
        "    def step2_async(self, state, actions):\n",
        "        self.venv.step2_async(state, actions)\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step_wait(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step2_wait(self):\n",
        "        pass\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        return self.venv.seed(seed)\n",
        "\n",
        "    def close(self):\n",
        "        return self.venv.close()\n",
        "\n",
        "    def render(self, mode: str = 'human'):\n",
        "        return self.venv.render(mode=mode)\n",
        "\n",
        "    def get_images(self):\n",
        "        return self.venv.get_images()\n",
        "\n",
        "    def get_attr(self, attr_name, indices=None):\n",
        "        return self.venv.get_attr(attr_name, indices)\n",
        "\n",
        "    def set_attr(self, attr_name, value, indices=None):\n",
        "        return self.venv.set_attr(attr_name, value, indices)\n",
        "\n",
        "    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):\n",
        "        return self.venv.env_method(method_name, *method_args, indices=indices, **method_kwargs)\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        \"\"\"Find attribute from wrapped venv(s) if this wrapper does not have it.\n",
        "        Useful for accessing attributes from venvs which are wrapped with multiple wrappers\n",
        "        which have unique attributes of interest.\n",
        "        \"\"\"\n",
        "        blocked_class = self.getattr_depth_check(name, already_found=False)\n",
        "        if blocked_class is not None:\n",
        "            own_class = \"{0}.{1}\".format(type(self).__module__, type(self).__name__)\n",
        "            format_str = (\"Error: Recursive attribute lookup for {0} from {1} is \"\n",
        "                          \"ambiguous and hides attribute from {2}\")\n",
        "            raise AttributeError(format_str.format(name, own_class, blocked_class))\n",
        "\n",
        "        return self.getattr_recursive(name)\n",
        "\n",
        "    def _get_all_attributes(self):\n",
        "        \"\"\"Get all (inherited) instance and class attributes\n",
        "\n",
        "        :return: (dict<str, object>) all_attributes\n",
        "        \"\"\"\n",
        "        all_attributes = self.__dict__.copy()\n",
        "        all_attributes.update(self.class_attributes)\n",
        "        return all_attributes\n",
        "\n",
        "    def getattr_recursive(self, name):\n",
        "        \"\"\"Recursively check wrappers to find attribute.\n",
        "\n",
        "        :param name (str) name of attribute to look for\n",
        "        :return: (object) attribute\n",
        "        \"\"\"\n",
        "        all_attributes = self._get_all_attributes()\n",
        "        if name in all_attributes:  # attribute is present in this wrapper\n",
        "            attr = getattr(self, name)\n",
        "        elif hasattr(self.venv, 'getattr_recursive'):\n",
        "            # Attribute not present, child is wrapper. Call getattr_recursive rather than getattr\n",
        "            # to avoid a duplicate call to getattr_depth_check.\n",
        "            attr = self.venv.getattr_recursive(name)\n",
        "        else:  # attribute not present, child is an unwrapped VecEnv\n",
        "            attr = getattr(self.venv, name)\n",
        "\n",
        "        return attr\n",
        "\n",
        "    def getattr_depth_check(self, name, already_found):\n",
        "        \"\"\"See base class.\n",
        "\n",
        "        :return: (str or None) name of module whose attribute is being shadowed, if any.\n",
        "        \"\"\"\n",
        "        all_attributes = self._get_all_attributes()\n",
        "        if name in all_attributes and already_found:\n",
        "            # this venv's attribute is being hidden because of a higher venv.\n",
        "            shadowed_wrapper_class = \"{0}.{1}\".format(type(self).__module__, type(self).__name__)\n",
        "        elif name in all_attributes and not already_found:\n",
        "            # we have found the first reference to the attribute. Now check for duplicates.\n",
        "            shadowed_wrapper_class = self.venv.getattr_depth_check(name, True)\n",
        "        else:\n",
        "            # this wrapper does not have the attribute. Keep searching.\n",
        "            shadowed_wrapper_class = self.venv.getattr_depth_check(name, already_found)\n",
        "\n",
        "        return shadowed_wrapper_class\n",
        "\n",
        "\n",
        "class CloudpickleWrapper(object):\n",
        "    def __init__(self, var):\n",
        "        \"\"\"\n",
        "        Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
        "\n",
        "        :param var: (Any) the variable you wish to wrap for pickling with cloudpickle\n",
        "        \"\"\"\n",
        "        self.var = var\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return cloudpickle.dumps(self.var)\n",
        "\n",
        "    def __setstate__(self, obs):\n",
        "        self.var = cloudpickle.loads(obs)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyqAvC9fKL0I",
        "outputId": "654eab91-7232-4683-c115-6028ceb9ef4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "VecEnv.step2"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.VecEnv.step2>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGT8HkJhFXqW"
      },
      "source": [
        "dummy_vec_env.py에 step2추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoKhgp9CFWo6",
        "cellView": "form"
      },
      "source": [
        "#@title dummy_vec_env.py\n",
        "from collections import OrderedDict\n",
        "from typing import Sequence\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines.common.vec_env.util import copy_obs_dict, dict_to_obs, obs_space_info\n",
        "\n",
        "\n",
        "class DummyVecEnv(VecEnv):\n",
        "    \"\"\"\n",
        "    Creates a simple vectorized wrapper for multiple environments, calling each environment in sequence on the current\n",
        "    Python process. This is useful for computationally simple environment such as ``cartpole-v1``, as the overhead of\n",
        "    multiprocess or multithread outweighs the environment computation time. This can also be used for RL methods that\n",
        "    require a vectorized environment, but that you want a single environments to train with.\n",
        "\n",
        "    :param env_fns: ([callable]) A list of functions that will create the environments\n",
        "        (each callable returns a `Gym.Env` instance when called).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_fns):\n",
        "        self.envs = [fn() for fn in env_fns]\n",
        "        env = self.envs[0]\n",
        "        VecEnv.__init__(self, len(env_fns), env.observation_space, env.action_space)\n",
        "        obs_space = env.observation_space\n",
        "        self.keys, shapes, dtypes = obs_space_info(obs_space)\n",
        "\n",
        "        self.buf_obs = OrderedDict([\n",
        "            (k, np.zeros((self.num_envs,) + tuple(shapes[k]), dtype=dtypes[k]))\n",
        "            for k in self.keys])\n",
        "        self.buf_dones = np.zeros((self.num_envs,), dtype=np.bool)\n",
        "        self.buf_rews = np.zeros((self.num_envs,), dtype=np.float32)\n",
        "        self.buf_infos = [{} for _ in range(self.num_envs)]\n",
        "        self.actions = None\n",
        "        self.metadata = env.metadata\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        self.actions = actions\n",
        "\n",
        "    def step2_async(self, state, actions):\n",
        "        self.actions = actions\n",
        "        self.state = state\n",
        "\n",
        "    def step_wait(self):\n",
        "        for env_idx in range(self.num_envs):\n",
        "            obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] =\\\n",
        "                self.envs[env_idx].step(self.actions[env_idx])\n",
        "            if self.buf_dones[env_idx]:\n",
        "                # save final observation where user can get it, then reset\n",
        "                self.buf_infos[env_idx]['terminal_observation'] = obs\n",
        "                obs = self.envs[env_idx].reset()\n",
        "            self._save_obs(env_idx, obs)\n",
        "        return (self._obs_from_buf(), np.copy(self.buf_rews), np.copy(self.buf_dones),\n",
        "                deepcopy(self.buf_infos))\n",
        "        \n",
        "    def step2_wait(self):\n",
        "        for env_idx in range(self.num_envs):\n",
        "            obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] =\\\n",
        "                self.envs[env_idx].step2(self.state[env_idx], self.actions[env_idx])\n",
        "            if self.buf_dones[env_idx]:\n",
        "                # save final observation where user can get it, then reset\n",
        "                self.buf_infos[env_idx]['terminal_observation'] = obs\n",
        "                obs = self.envs[env_idx].reset()\n",
        "            self._save_obs(env_idx, obs)\n",
        "        return (self._obs_from_buf(), np.copy(self.buf_rews), np.copy(self.buf_dones),\n",
        "                deepcopy(self.buf_infos))\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        seeds = list()\n",
        "        for idx, env in enumerate(self.envs):\n",
        "            seeds.append(env.seed(seed + idx))\n",
        "        return seeds\n",
        "\n",
        "    def reset(self):\n",
        "        for env_idx in range(self.num_envs):\n",
        "            obs = self.envs[env_idx].reset()\n",
        "            self._save_obs(env_idx, obs)\n",
        "        return self._obs_from_buf()\n",
        "\n",
        "    def close(self):\n",
        "        for env in self.envs:\n",
        "            env.close()\n",
        "\n",
        "    def get_images(self) -> Sequence[np.ndarray]:\n",
        "        return [env.render(mode='rgb_array') for env in self.envs]\n",
        "\n",
        "    def render(self, mode: str = 'human'):\n",
        "        \"\"\"\n",
        "        Gym environment rendering. If there are multiple environments then\n",
        "        they are tiled together in one image via `BaseVecEnv.render()`.\n",
        "        Otherwise (if `self.num_envs == 1`), we pass the render call directly to the\n",
        "        underlying environment.\n",
        "\n",
        "        Therefore, some arguments such as `mode` will have values that are valid\n",
        "        only when `num_envs == 1`.\n",
        "\n",
        "        :param mode: The rendering type.\n",
        "        \"\"\"\n",
        "        if self.num_envs == 1:\n",
        "            return self.envs[0].render(mode=mode)\n",
        "        else:\n",
        "            return super().render(mode=mode)\n",
        "\n",
        "    def _save_obs(self, env_idx, obs):\n",
        "        for key in self.keys:\n",
        "            if key is None:\n",
        "                self.buf_obs[key][env_idx] = obs\n",
        "            else:\n",
        "                self.buf_obs[key][env_idx] = obs[key]\n",
        "\n",
        "    def _obs_from_buf(self):\n",
        "        return dict_to_obs(self.observation_space, copy_obs_dict(self.buf_obs))\n",
        "\n",
        "    def get_attr(self, attr_name, indices=None):\n",
        "        \"\"\"Return attribute from vectorized environment (see base class).\"\"\"\n",
        "        target_envs = self._get_target_envs(indices)\n",
        "        return [getattr(env_i, attr_name) for env_i in target_envs]\n",
        "\n",
        "    def set_attr(self, attr_name, value, indices=None):\n",
        "        \"\"\"Set attribute inside vectorized environments (see base class).\"\"\"\n",
        "        target_envs = self._get_target_envs(indices)\n",
        "        for env_i in target_envs:\n",
        "            setattr(env_i, attr_name, value)\n",
        "\n",
        "    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):\n",
        "        \"\"\"Call instance methods of vectorized environments.\"\"\"\n",
        "        target_envs = self._get_target_envs(indices)\n",
        "        return [getattr(env_i, method_name)(*method_args, **method_kwargs) for env_i in target_envs]\n",
        "\n",
        "    def _get_target_envs(self, indices):\n",
        "        indices = self._get_indices(indices)\n",
        "        return [self.envs[i] for i in indices]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLBSIFl2KT_4",
        "outputId": "249e8145-98a8-4aaa-93d9-8d368ec38fa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "DummyVecEnv.step2"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.VecEnv.step2>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoPDOduZK4Wr",
        "cellView": "form"
      },
      "source": [
        "#@title make_vec_env\n",
        "\"\"\"\n",
        "Helpers for scripts like run_atari.py.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import gym\n",
        "\n",
        "from stable_baselines import logger\n",
        "from stable_baselines.bench import Monitor\n",
        "from stable_baselines.common.misc_util import set_global_seeds\n",
        "from stable_baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
        "from stable_baselines.common.misc_util import mpi_rank_or_zero\n",
        "from stable_baselines.common.vec_env import SubprocVecEnv\n",
        "\n",
        "\n",
        "def make_vec_env(env_id, n_envs=1, seed=None, start_index=0,\n",
        "                 monitor_dir=None, wrapper_class=None,\n",
        "                 env_kwargs=None, vec_env_cls=None, vec_env_kwargs=None):\n",
        "    \"\"\"\n",
        "    Create a wrapped, monitored `VecEnv`.\n",
        "    By default it uses a `DummyVecEnv` which is usually faster\n",
        "    than a `SubprocVecEnv`.\n",
        "\n",
        "    :param env_id: (str or Type[gym.Env]) the environment ID or the environment class\n",
        "    :param n_envs: (int) the number of environments you wish to have in parallel\n",
        "    :param seed: (int) the initial seed for the random number generator\n",
        "    :param start_index: (int) start rank index\n",
        "    :param monitor_dir: (str) Path to a folder where the monitor files will be saved.\n",
        "        If None, no file will be written, however, the env will still be wrapped\n",
        "        in a Monitor wrapper to provide additional information about training.\n",
        "    :param wrapper_class: (gym.Wrapper or callable) Additional wrapper to use on the environment.\n",
        "        This can also be a function with single argument that wraps the environment in many things.\n",
        "    :param env_kwargs: (dict) Optional keyword argument to pass to the env constructor\n",
        "    :param vec_env_cls: (Type[VecEnv]) A custom `VecEnv` class constructor. Default: None.\n",
        "    :param vec_env_kwargs: (dict) Keyword arguments to pass to the `VecEnv` class constructor.\n",
        "    :return: (VecEnv) The wrapped environment\n",
        "    \"\"\"\n",
        "    env_kwargs = {} if env_kwargs is None else env_kwargs\n",
        "    vec_env_kwargs = {} if vec_env_kwargs is None else vec_env_kwargs\n",
        "\n",
        "    def make_env(rank):\n",
        "        def _init():\n",
        "            if isinstance(env_id, str):\n",
        "                env = gym.make(env_id)\n",
        "                if len(env_kwargs) > 0:\n",
        "                    warnings.warn(\"No environment class was passed (only an env ID) so `env_kwargs` will be ignored\")\n",
        "            else:\n",
        "                env = env_id(**env_kwargs)\n",
        "            if seed is not None:\n",
        "                env.seed(seed + rank)\n",
        "                env.action_space.seed(seed + rank)\n",
        "            # Wrap the env in a Monitor wrapper\n",
        "            # to have additional training information\n",
        "            monitor_path = os.path.join(monitor_dir, str(rank)) if monitor_dir is not None else None\n",
        "            # Create the monitor folder if needed\n",
        "            if monitor_path is not None:\n",
        "                os.makedirs(monitor_dir, exist_ok=True)\n",
        "            env = Monitor(env, filename=monitor_path)\n",
        "            # Optionally, wrap the environment with the provided wrapper\n",
        "            if wrapper_class is not None:\n",
        "                env = wrapper_class(env)\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    # No custom VecEnv is passed\n",
        "    if vec_env_cls is None:\n",
        "        # Default: use a DummyVecEnv\n",
        "        vec_env_cls = DummyVecEnv\n",
        "\n",
        "    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nkDPBWV_88O"
      },
      "source": [
        "source/target 환경 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAkQcFrI_e5H"
      },
      "source": [
        "source_env = make_vec_env(CartPoleEnv_source, n_envs = 1, seed = 1000)\n",
        "target_env = make_vec_env(CartPoleEnv, n_envs= 1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l31wCKu5KuEG",
        "outputId": "17cd6e76-4281-4370-e323-0c74fabc9039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "source_env.step2"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method VecEnv.step2 of <__main__.DummyVecEnv object at 0x7fc54e34a518>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfbnoSeTAQa4"
      },
      "source": [
        "Policy_IS\n",
        "- policy fn.은 deterministic argument로 인해 deterministic fn.으로 할지 probability fn.으로 할지 선택 가능해 보임\n",
        "- 그런데 왜 그걸 Done값으로 선택하는 지는 잘 모르겠음!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0TJX__1_2MG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from stable_baselines.common.policies import FeedForwardPolicy\n",
        "\n",
        "class policy_IS(FeedForwardPolicy):\n",
        "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **_kwargs):\n",
        "        super(policy_IS, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n",
        "                                        feature_extraction=\"mlp\", **_kwargs)\n",
        "\n",
        "    # q_value가 추가됨\n",
        "    # state, mask는 RNN 을 위한 것\n",
        "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
        "        '''\n",
        "\n",
        "        :param obs: ([float] or [int]) The current observation of the environment\n",
        "        :param state: ([float]) The last states (used in recurrent policies)\n",
        "        :param mask: ([float]) The last masks (used in recurrent policies)\n",
        "        :param deterministic: (bool) Whether or not to return deterministic actions.\n",
        "        :return: ([float], [float], [float], [float], [float]) actions, values, states, neglogp, q_value_acted\n",
        "        '''\n",
        "        if deterministic:\n",
        "            action, value, neglogp, q_value = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp, self.q_value],\n",
        "                                                   {self.obs_ph: obs})\n",
        "            # action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
        "            #                                        {self.obs_ph: obs})\n",
        "        else:\n",
        "            action, value, neglogp, q_value = self.sess.run([self.action, self.value_flat, self.neglogp, self.q_value],\n",
        "                                                   {self.obs_ph: obs})\n",
        "            # action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
        "            #                                        {self.obs_ph: obs})\n",
        "        q_value_acted = q_value[np.arange(len(q_value)), action]\n",
        "        return action, value, self.initial_state, neglogp, q_value_acted\n",
        "        # return action, value, self.initial_state, neglogp\n",
        "\n",
        "    def proba_step(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
        "\n",
        "    def value(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self.value_flat, {self.obs_ph: obs})"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWkYTaIqBe6j",
        "cellView": "form"
      },
      "source": [
        "#@title weight_calculator.py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from stable_baselines.common.input import observation_input\n",
        "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc\n",
        "\n",
        "\n",
        "class weight_calculator():\n",
        "    def __init__(self, sess, obs_space, l_rate, max_grad_norm = 0.5, method = 'Classifier', cnn_kwargs = None):\n",
        "        self.sess = sess\n",
        "        self.obs_space = obs_space\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.policy_kwargs = {} if cnn_kwargs is None else cnn_kwargs\n",
        "\n",
        "        # method = 'SR' or 'Classifier'\n",
        "        if method == 'SR':\n",
        "            # create tabular\n",
        "            pass\n",
        "        elif method == 'Classifier':\n",
        "            self.setup_classifier(l_rate, self.max_grad_norm, cnn_kwargs)\n",
        "\n",
        "    def setup_classifier(self, l_rate, max_grad_norm, layers = None, act_fun = tf.tanh, reuse = False, feature_extraction = \"mlp\", **kwargs):\n",
        "        with tf.compat.v1.variable_scope(\"classifier\", reuse=reuse):\n",
        "            # Build observation input with encoding depending on the observation space type\n",
        "            self.obs_src_ph, processed_obs_src = observation_input(self.obs_space, batch_size=None, name='ob_src_classifier',\n",
        "                                                              scale=(feature_extraction == \"cnn\"))\n",
        "            self.obs_trg_ph, processed_obs_trg = observation_input(self.obs_space, batch_size=None, name='ob_trg_classifier',\n",
        "                                                              scale=(feature_extraction == \"cnn\"))\n",
        "\n",
        "            # # Give noise\n",
        "            # input_noise = 0.1\n",
        "            # processed_obs_src = tf.keras.layers.GaussianNoise(input_noise)(processed_obs_src)\n",
        "            # processed_obs_trg = tf.keras.layers.GaussianNoise(input_noise)(processed_obs_trg)\n",
        "\n",
        "            classifier_input = tf.concat([processed_obs_src, processed_obs_trg], axis=0)\n",
        "            src_size = tf.shape(processed_obs_src)[0]\n",
        "            trg_size = tf.shape(processed_obs_trg)[0]\n",
        "            y_true = tf.concat([tf.zeros(src_size, dtype=tf.float32), tf.ones(trg_size, dtype=tf.float32)], axis=0) # source == 0, target == 1\n",
        "\n",
        "            # Build NN\n",
        "            if layers is None:\n",
        "                layers = [64, 64]\n",
        "\n",
        "            if feature_extraction == \"cnn\":\n",
        "                classfier_latent = self.nature_cnn(classifier_input, **kwargs)\n",
        "            else:\n",
        "                classfier_latent = classifier_input\n",
        "                for idx, (layer_size) in enumerate(layers):\n",
        "                    assert isinstance(layer_size, int), \"Error: net_arch[-1]['classifier'] must only contain integers.\"\n",
        "                    classfier_latent = act_fun(linear(classfier_latent, \"classifier{}\".format(idx),\n",
        "                                                      layer_size, init_scale=np.sqrt(2)))\n",
        "\n",
        "            # # One-dimensional logit\n",
        "            # logits = tf.squeeze(linear(classfier_latent, 'logit', 1))\n",
        "            # self.prob = tf.nn.sigmoid(logits) # softmax or sigmoid..?\n",
        "            #\n",
        "            # # Define loss function: cross_entropy\n",
        "            # classifier_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = y_true)\n",
        "            # classifier_loss = tf.reduce_mean(classifier_loss)\n",
        "            # # self.generator_acc = tf.reduce_mean(tf.to_float(tf.nn.sigmoid(generator_logits) < 0.5))\n",
        "            # # self.expert_acc = tf.reduce_mean(tf.to_float(tf.nn.sigmoid(expert_logits) > 0.5))\n",
        "            # # Build entropy loss\n",
        "            # entropy = tf.reduce_mean(self.logit_bernoulli_entropy(logits))\n",
        "            # entropy_loss = -0.001 * entropy\n",
        "            # self.total_loss = classifier_loss + entropy_loss\n",
        "\n",
        "            # Two logits\n",
        "            logits = linear(classfier_latent, 'logit', 2)\n",
        "            probs = tf.nn.softmax(logits, name=\"probs\")\n",
        "            # probs = tf.nn.sigmoid(logits, name=\"probs\")\n",
        "            self.prob = probs[:,1] / probs[:,0]\n",
        "            self.total_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, probs)\n",
        "\n",
        "            # Operator for updating classifier\n",
        "            self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=l_rate)\n",
        "\n",
        "            self.tvars = tf.trainable_variables(\"classifier\")\n",
        "            self.gradients = tf.gradients(self.total_loss, self.tvars)\n",
        "            if max_grad_norm is not None:\n",
        "                self.gradients, self.norm = tf.clip_by_global_norm(self.gradients, max_grad_norm)\n",
        "            self.grads = list(zip(self.gradients, self.tvars))\n",
        "            # update network by getting cumulative gradients\n",
        "            self.updateModel = self.optimizer.apply_gradients(self.grads)\n",
        "\n",
        "            # tf.summary.scalar('total_loss_classifier', self.total_loss)\n",
        "            # tf.summary.scalar('classifier_loss', classifier_loss)\n",
        "\n",
        "    def update_visitations(self, obs_src, obs_trg):\n",
        "        feed_dict = {self.obs_src_ph: obs_src, self.obs_trg_ph: obs_trg}\n",
        "        if self.max_grad_norm is not None:\n",
        "            _, loss, global_norm = self.sess.run([self.updateModel, self.total_loss, self.norm], feed_dict)\n",
        "        else:\n",
        "            _, loss = self.sess.run([self.updateModel, self.total_loss], feed_dict)\n",
        "            global_norm = 0\n",
        "        return loss, global_norm\n",
        "\n",
        "    def get_IS(self, obs_src):\n",
        "        feed_dict = {self.obs_src_ph: obs_src, self.obs_trg_ph: obs_src}\n",
        "        return self.sess.run(self.prob, feed_dict)\n",
        "\n",
        "    def nature_cnn(self, scaled_images, **kwargs):\n",
        "        \"\"\"\n",
        "        CNN from Nature paper.\n",
        "\n",
        "        :param scaled_images: (TensorFlow Tensor) Image input placeholder\n",
        "        :param kwargs: (dict) Extra keywords parameters for the convolutional layers of the CNN\n",
        "        :return: (TensorFlow Tensor) The CNN output layer\n",
        "        \"\"\"\n",
        "        activ = tf.nn.relu\n",
        "        layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=8, stride=4, init_scale=np.sqrt(2), **kwargs))\n",
        "        layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=4, stride=2, init_scale=np.sqrt(2), **kwargs))\n",
        "        layer_3 = activ(conv(layer_2, 'c3', n_filters=64, filter_size=3, stride=1, init_scale=np.sqrt(2), **kwargs))\n",
        "        layer_3 = conv_to_fc(layer_3)\n",
        "        return activ(linear(layer_3, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))\n",
        "\n",
        "    def logsigmoid(self, a):\n",
        "        '''Equivalent to tf.log(tf.sigmoid(a))'''\n",
        "        return -tf.nn.softplus(-a)\n",
        "\n",
        "    \"\"\" Reference: https://github.com/openai/imitation/blob/99fbccf3e060b6e6c739bdf209758620fcdefd3c/policyopt/thutil.py#L48-L51\"\"\"\n",
        "\n",
        "    def logit_bernoulli_entropy(self, logits):\n",
        "        ent = (1. - tf.nn.sigmoid(logits)) * logits - self.logsigmoid(logits)\n",
        "        return ent"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkHEx9jXBQlT"
      },
      "source": [
        "A2C_IS\n",
        "- colab에 설치된 stable-baseline에서는 argument에 momentum이 있으면 에러가 나서 제거함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vDasNYdArdT",
        "cellView": "form"
      },
      "source": [
        "#@title A2C_IS.py\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.spaces.space import Space\n",
        "from gym.spaces.box import Box\n",
        "from gym.spaces.discrete import Discrete\n",
        "\n",
        "from stable_baselines import A2C\n",
        "from stable_baselines import logger\n",
        "from stable_baselines.common import explained_variance, tf_util, ActorCriticRLModel, SetVerbosity, TensorboardWriter\n",
        "from stable_baselines.common.runners import AbstractEnvRunner\n",
        "from stable_baselines.common.schedules import Scheduler\n",
        "from stable_baselines.common.tf_util import mse, total_episode_reward_logger\n",
        "from stable_baselines.common.math_util import safe_mean\n",
        "from stable_baselines.common.vec_env import VecNormalize, unwrap_vec_normalize\n",
        "from stable_baselines.common.base_class import _UnvecWrapper\n",
        "from stable_baselines.common.policies import ActorCriticPolicy, RecurrentActorCriticPolicy\n",
        "# from stable_baselines.common.buffers import ReplayBuffer\n",
        "\n",
        "#from A2C_IS_Runner import A2C_IS_Runner\n",
        "#from weight_calculator import weight_calculator\n",
        "\n",
        "\n",
        "class A2C_IS(A2C):\n",
        "\n",
        "    def __init__(self, policy, env, source_env = None,source_demo = None, gamma=0.99, n_steps=5, vf_coef=0.25, ent_coef=0.01, max_grad_norm=0.5,\n",
        "                 learning_rate=7e-4, alpha=0.99, epsilon=1e-5, lr_schedule='constant',\n",
        "                 verbose=0, tensorboard_log=None, _init_setup_model=True, policy_kwargs=None,\n",
        "                 full_tensorboard_log=False, seed=None, n_cpu_tf_sess=None):\n",
        "\n",
        "        # # Initialize evaluation environment\n",
        "        # self.eval_env = eval_env\n",
        "\n",
        "        # Initialize source_env\n",
        "        self.source_env = source_env\n",
        "        self.source_demo = source_demo\n",
        "        # source optimal policy를 활용할 때 사용하는 if문\n",
        "        if self.source_demo is not None:\n",
        "            self.threshold = 0.5 # dynamics가 어느 정도 다를 때 안쓸 것인가? 쓸 것인가?\n",
        "            # demonstration 저장된 거 불러와서 읽어들이는 부분\n",
        "            self.actions_demo = source_demo['actions']\n",
        "            self.starts_demo = source_demo['episode_starts']\n",
        "            self.obs_demo = source_demo['obs']\n",
        "            self.rewards_demo = source_demo['rewards']\n",
        "            # temp2 = source_demo['episode_returns']\n",
        "            self.n_demo_samples = np.shape(self.obs_demo)[0]\n",
        "            # self.src_buffer = ReplayBuffer(np.shape(self.obs_demo)[0])\n",
        "        self._source_runner = None\n",
        "\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.learning_rate_c = learning_rate\n",
        "        # self.learning_rate_c = 0.001\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        super(A2C_IS, self).__init__(policy=policy, env=env, gamma=gamma, n_steps=n_steps, vf_coef=vf_coef, ent_coef=ent_coef, max_grad_norm=max_grad_norm,\n",
        "                     learning_rate=learning_rate, alpha=alpha, epsilon=epsilon, lr_schedule=lr_schedule,\n",
        "                     verbose=verbose, tensorboard_log=tensorboard_log, _init_setup_model=_init_setup_model, policy_kwargs=policy_kwargs,\n",
        "                     full_tensorboard_log=full_tensorboard_log, seed=seed, n_cpu_tf_sess=n_cpu_tf_sess)\n",
        "    # Target sample 만드는 부분\n",
        "    def _make_runner(self) -> AbstractEnvRunner:\n",
        "        return A2C_IS_Runner(self.env, self, run_target=True, n_steps=self.n_steps, gamma=self.gamma)\n",
        "\n",
        "    @property\n",
        "    def source_runner(self) -> AbstractEnvRunner:\n",
        "        if self._source_runner is None:\n",
        "            self._source_runner = self._make_source_runner()\n",
        "        return self._source_runner\n",
        "    # Source sample 만드는 부분\n",
        "    def _make_source_runner(self) -> AbstractEnvRunner:\n",
        "        return A2C_IS_Runner(self.source_env, self, run_target=False, n_steps=self.n_steps, gamma=self.gamma)\n",
        "\n",
        "    def learn(self, total_timesteps, callback=None, log_interval=100, tb_log_name=\"A2C\",\n",
        "              reset_num_timesteps=True):\n",
        "\n",
        "        new_tb_log = self._init_num_timesteps(reset_num_timesteps)\n",
        "        callback = self._init_callback(callback)\n",
        "\n",
        "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, tb_log_name, new_tb_log) \\\n",
        "                as writer:\n",
        "            self._setup_learn()\n",
        "            self.learning_rate_schedule = Scheduler(initial_value=self.learning_rate, n_values=total_timesteps,\n",
        "                                                    schedule=self.lr_schedule)\n",
        "\n",
        "            t_start = time.time()\n",
        "            callback.on_training_start(locals(), globals())\n",
        "\n",
        "            # num_target_samples = 0\n",
        "\n",
        "            for update in range(1, total_timesteps // self.n_batch + 1):\n",
        "\n",
        "                callback.on_rollout_start()\n",
        "                # true_reward is the reward without discount\n",
        "                rollout = self.runner.run(callback)\n",
        "                # unpack\n",
        "                obs_t, states_t, rewards_t, masks_t, actions_t, values_t, q_values_t, ep_infos, true_reward_t, steps = rollout\n",
        "\n",
        "                # num_target_samples += np.size(rewards_t)\n",
        "\n",
        "                if self.source_env is not None: # Integrate source samples\n",
        "                    rollout_source = self.source_runner.run(callback)\n",
        "                    obs_s, states_s, rewards_s, masks_s, actions_s, values_s, q_values_s, ep_infos_s, true_reward_s, steps = rollout_source\n",
        "                    n_source_samples = np.shape(obs_s)[0]\n",
        "                    n_opt_samples = n_source_samples\n",
        "\n",
        "                    if self.source_demo is not None:\n",
        "                        obs_s_opt, n_obs_s_opt, rewards_s_opt, dones_opt, actions_s_opt, values_s_opt = [], [], [], [], [], []\n",
        "                        # idx = 0\n",
        "                        n_opt_samples = int(n_source_samples * np.power(0.999, update))\n",
        "                        # print(n_opt_samples)\n",
        "                        for n in range(n_opt_samples):\n",
        "                            idx = np.random.randint(self.n_demo_samples)\n",
        "                            obs_s_opt.append(self.obs_demo[idx])\n",
        "                            rewards_s_opt.append(self.rewards_demo[idx])\n",
        "                            actions_s_opt.append(self.actions_demo[idx])\n",
        "                            if idx == self.n_demo_samples -1:\n",
        "                                dones_opt.append(True)\n",
        "                                n_obs_s_opt.append(self.obs_demo[idx])\n",
        "                            else:\n",
        "                                dones_opt.append(self.starts_demo[idx + 1])\n",
        "                                n_obs_s_opt.append(self.obs_demo[idx + 1])\n",
        "                            # idx += 1\n",
        "                        if n_opt_samples is not 0:\n",
        "                            obs_s_opt = np.asarray(obs_s_opt, dtype=self.observation_space.dtype).swapaxes(1, 0).reshape((n_opt_samples,) + self.observation_space.shape)\n",
        "                            actions_s_opt = np.asarray(actions_s_opt, dtype=self.action_space.dtype)\n",
        "                            rewards_s_opt = np.asarray(rewards_s_opt, dtype=np.float32)\n",
        "                            dones_opt = np.asarray(dones_opt, dtype=np.bool)\n",
        "\n",
        "                            # Rewards, Value computation\n",
        "                            _, value, _, _ = self.step(obs = np.append(obs_s_opt, n_obs_s_opt, axis = 0))\n",
        "                            values_s_opt = value[:n_opt_samples]\n",
        "                            for i in range(len(rewards_s_opt)):\n",
        "                                rewards_s_opt[i] = rewards_s_opt[i] + self.gamma * value[n_opt_samples + i] * (1. - dones_opt[i])\n",
        "                            # Integrate inputs\n",
        "                            obs_classifier = np.append(obs_s, obs_s_opt, axis = 0)\n",
        "                        else:\n",
        "                            obs_classifier = obs_s\n",
        "                    else:\n",
        "                        obs_classifier = obs_s\n",
        "\n",
        "                    # Compute importance weight\n",
        "                    # input: obs, obs_s, output: weight\n",
        "                    # weights = np.nan_to_num(probs_to_be_trg / (1-probs_to_be_trg), nan = 1.7976931348623157e+308)\n",
        "                    # probs_to_be_trg = np.float64(probs_to_be_trg)\n",
        "                    probs_to_be_trg = self.classifier.get_IS(obs_src=obs_classifier)[0:np.shape(obs_classifier)[0]]\n",
        "\n",
        "                    # With 1-D logit classifier\n",
        "                    # with np.errstate(divide='raise'):\n",
        "                    #     try:\n",
        "                    #         weights = probs_to_be_trg / (1-probs_to_be_trg)\n",
        "                    #     except:\n",
        "                    #         probs_to_be_trg[probs_to_be_trg == 1.0] = 1.0 - 1.0e-10\n",
        "\n",
        "                    # Without IS\n",
        "                    # weights = np.ones_like(probs_to_be_trg)\n",
        "\n",
        "                    # With 2-D logit classifier\n",
        "                    weights = probs_to_be_trg # two-dimensional logit\n",
        "\n",
        "                    # Weight clipped by 1\n",
        "                    # weights = np.minimum(weights, np.ones_like(weights))\n",
        "\n",
        "                    # # Case when doesn't use estimated ADV\n",
        "                    # weighted_rewards = rewards_s * weights[0:n_source_samples]\n",
        "                    # weighted_values = values_s * weights[0:n_source_samples]\n",
        "\n",
        "                    # Case when use estimated ADV\n",
        "                    # weighted_rewards = q_values_s * weights[0:n_source_samples]\n",
        "                    # weighted_values = np.zeros_like(weighted_rewards)\n",
        "                    weighted_rewards = rewards_s\n",
        "                    weighted_values = values_s\n",
        "                    q_values_s = q_values_s * weights[0:n_source_samples]\n",
        "\n",
        "                    # Threshold..\n",
        "                    if self.source_demo is not None and n_opt_samples is not 0:\n",
        "                        delete_rows = []\n",
        "                        for i in range(n_opt_samples):\n",
        "                            if np.abs(weights[n_source_samples + i]-1) > self.threshold:\n",
        "                                delete_rows.append(i)\n",
        "                        obs_s_opt = np.delete(obs_s_opt,delete_rows,axis=0)\n",
        "                        actions_s_opt = np.delete(actions_s_opt, delete_rows, axis=0)\n",
        "                        rewards_s_opt = np.delete(rewards_s_opt[:,0] * weights[n_source_samples:], delete_rows, axis=0)\n",
        "                        values_s_opt = np.delete(values_s_opt * weights[n_source_samples:], delete_rows, axis=0)\n",
        "\n",
        "                        obs_s = np.append(obs_s, obs_s_opt, axis = 0)\n",
        "                        actions_s = np.append(actions_s, actions_s_opt)\n",
        "                        weighted_rewards = np.append(weighted_rewards, rewards_s_opt)\n",
        "                        weighted_values = np.append(weighted_values, values_s_opt)\n",
        "\n",
        "                        # Test case: use only opt src\n",
        "                        # obs_s = obs_s_opt\n",
        "                        # actions_s = actions_s_opt\n",
        "                        # weighted_rewards = rewards_s_opt\n",
        "                        # weighted_values = values_s_opt\n",
        "\n",
        "                    # Integrate inputs (target + source)\n",
        "                    obs = np.append(obs_t, obs_s, axis = 0)\n",
        "                    masks = np.append(masks_t, masks_s)\n",
        "                    actions = np.append(actions_t, actions_s)\n",
        "                    if states_t is not None:\n",
        "                        print(\"RNN policy is not applicable\")\n",
        "                    else:\n",
        "                        states = None\n",
        "                    rewards = np.append(rewards_t, weighted_rewards)\n",
        "                    values = np.append(values_t, weighted_values)\n",
        "                    q_values = np.append(q_values_t, q_values_s)\n",
        "                    # ep_infos = np.append(ep_infos_t, ep_infos_s)\n",
        "                    # true_reward = np.append(true_reward_t, true_reward_s)\n",
        "\n",
        "                else:\n",
        "                    obs, states, rewards, masks, actions, values, q_values, ep_infos, true_reward_t, steps = rollout\n",
        "\n",
        "                callback.update_locals(locals())\n",
        "                callback.on_rollout_end()\n",
        "\n",
        "                # Early stopping due to the callback\n",
        "                if not self.runner.continue_training:\n",
        "                    break\n",
        "\n",
        "                self.ep_info_buf.extend(ep_infos)\n",
        "\n",
        "                # Update classifer, actor, critic\n",
        "                if self.source_env is not None:\n",
        "                    classifier_loss, _ = self.classifier.update_visitations(obs_s[0:n_source_samples], obs_t)\n",
        "                else:\n",
        "                    classifier_loss = 0\n",
        "                _, value_loss, policy_entropy = self._train_step(obs, states, rewards, masks, actions, values, q_values, np.shape(rewards_t)[0],\n",
        "                                                                 self.num_timesteps // self.n_batch, writer)\n",
        "                # # Update value also with source samples\n",
        "                # _, value_loss, policy_entropy = self._train_step(obs, states, rewards, masks, actions, values, q_values, np.shape(obs)[0],\n",
        "                #                                                  self.num_timesteps // self.n_batch, writer)\n",
        "\n",
        "                # Save logs\n",
        "                n_seconds = time.time() - t_start\n",
        "                fps = int((update * self.n_batch) / n_seconds)\n",
        "\n",
        "                if writer is not None:\n",
        "                    # Evaluation per update\n",
        "                    # if num_target_samples%(self.n_batch*self.n_envs) == 0:\n",
        "                    # if num_target_samples % 1280 == 0:\n",
        "                    #     episodes_to_eval_per_update = 10\n",
        "                    #     returns = 0\n",
        "                    #     for i in range(episodes_to_eval_per_update):\n",
        "                    #         obs_eval = self.eval_env.reset()\n",
        "                    #         dones = False\n",
        "                    #         cum_rewards = 0\n",
        "                    #         while not dones:\n",
        "                    #             action, _states = self.predict_customize(obs_eval)\n",
        "                    #             obs_eval, rewards_eval, dones, _ = self.eval_env.step(action)\n",
        "                    #             cum_rewards += rewards_eval\n",
        "                    #         returns += cum_rewards\n",
        "                    #     returns = returns / episodes_to_eval_per_update\n",
        "                    #\n",
        "                    #     with tf.variable_scope(\"evaluation\", reuse=True):\n",
        "                    #         summary = tf.Summary(\n",
        "                    #             value=[tf.compat.v1.Summary.Value(tag=\"episode reward (evaluation)\", simple_value=returns[0])])\n",
        "                    #         writer.add_summary(summary, self.num_timesteps)\n",
        "\n",
        "                    # Original method of obtaining episode reward: Record whenever episode ended\n",
        "                    total_episode_reward_logger(self.episode_reward,\n",
        "                                                true_reward_t.reshape((self.n_envs, self.n_steps)),\n",
        "                                                masks_t.reshape((self.n_envs, self.n_steps)),\n",
        "                                                writer, self.num_timesteps)\n",
        "                    if self.source_env is not None:\n",
        "                        with tf.variable_scope(\"classifier_info\", reuse=True):\n",
        "                            summary = tf.Summary(value=[tf.compat.v1.Summary.Value(tag=\"weights\", simple_value=np.mean(weights))])\n",
        "                            writer.add_summary(summary, self.num_timesteps)\n",
        "                            summary2 = tf.Summary(value=[tf.compat.v1.Summary.Value(tag=\"classifier_loss\", simple_value=np.mean(classifier_loss))])\n",
        "                            writer.add_summary(summary2, self.num_timesteps)\n",
        "\n",
        "                if self.verbose >= 1 and (update % log_interval == 0 or update == 1):\n",
        "                    explained_var = explained_variance(values, rewards)\n",
        "                    logger.record_tabular(\"nupdates\", update)\n",
        "                    logger.record_tabular(\"total_timesteps\", self.num_timesteps)\n",
        "                    logger.record_tabular(\"fps\", fps)\n",
        "                    logger.record_tabular(\"policy_entropy\", float(policy_entropy))\n",
        "                    logger.record_tabular(\"value_loss\", float(value_loss))\n",
        "                    if self.source_env is not None:\n",
        "                        logger.record_tabular(\"classifier_loss\", float(np.mean(classifier_loss)))\n",
        "                        logger.record_tabular(\"mean_weight\", float(np.mean(weights)))\n",
        "                    logger.record_tabular(\"explained_variance\", float(explained_var))\n",
        "                    if len(self.ep_info_buf) > 0 and len(self.ep_info_buf[0]) > 0:\n",
        "                        logger.logkv('ep_reward_mean', safe_mean([ep_info['r'] for ep_info in self.ep_info_buf]))\n",
        "                        logger.logkv('ep_len_mean', safe_mean([ep_info['l'] for ep_info in self.ep_info_buf]))\n",
        "                    logger.dump_tabular()\n",
        "\n",
        "            # print(\"Total target samples: \", num_target_samples)\n",
        "\n",
        "        callback.on_training_end()\n",
        "        return self\n",
        "\n",
        "    def setup_model(self):\n",
        "        with SetVerbosity(self.verbose):\n",
        "\n",
        "            assert issubclass(self.policy, ActorCriticPolicy), \"Error: the input policy for the A2C model must be an \" \\\n",
        "                                                                \"instance of common.policies.ActorCriticPolicy.\"\n",
        "\n",
        "            self.graph = tf.Graph()\n",
        "            with self.graph.as_default():\n",
        "                self.set_random_seed(self.seed)\n",
        "                self.sess = tf_util.make_session(num_cpu=self.n_cpu_tf_sess, graph=self.graph)\n",
        "\n",
        "                self.n_batch = self.n_envs * self.n_steps\n",
        "\n",
        "                n_batch_step = None\n",
        "                n_batch_train = None\n",
        "                if issubclass(self.policy, RecurrentActorCriticPolicy):\n",
        "                    n_batch_step = self.n_envs\n",
        "                    n_batch_train = self.n_envs * self.n_steps\n",
        "\n",
        "                # Build classifier\n",
        "                if self.source_env is not None:\n",
        "                    classifier = weight_calculator(sess = self.sess, obs_space = self.observation_space, l_rate=self.learning_rate_c,\n",
        "                                                   max_grad_norm=self.max_grad_norm, **self.policy_kwargs)\n",
        "\n",
        "                step_model = self.policy(self.sess, self.observation_space, self.action_space, self.n_envs, 1,\n",
        "                                         n_batch_step, reuse=False, **self.policy_kwargs)\n",
        "\n",
        "                with tf.variable_scope(\"train_model\", reuse=True,\n",
        "                                       custom_getter=tf_util.outer_scope_getter(\"train_model\")):\n",
        "                    train_model = self.policy(self.sess, self.observation_space, self.action_space, self.n_envs,\n",
        "                                              self.n_steps, n_batch_train, reuse=True, **self.policy_kwargs)\n",
        "\n",
        "                with tf.variable_scope(\"loss\", reuse=False):\n",
        "                    self.actions_ph = train_model.pdtype.sample_placeholder([None], name=\"action_ph\")\n",
        "                    self.advs_ph = tf.placeholder(tf.float32, [None], name=\"advs_ph\")\n",
        "                    self.rewards_ph = tf.placeholder(tf.float32, [None], name=\"rewards_ph\")\n",
        "                    self.learning_rate_ph = tf.placeholder(tf.float32, [], name=\"learning_rate_ph\")\n",
        "\n",
        "                    neglogpac = train_model.proba_distribution.neglogp(self.actions_ph)\n",
        "                    self.entropy = tf.reduce_mean(train_model.proba_distribution.entropy())\n",
        "                    self.pg_loss = tf.reduce_mean(self.advs_ph * neglogpac)\n",
        "\n",
        "                    # self.obs_value_in = tf.slice(tf.squeeze(train_model.value_flat), [0], [tf.shape(self.rewards_ph)[0]])\n",
        "                    # self.vf_loss = mse(tf.squeeze(train_model.value_flat), self.rewards_ph) # Original, Estimate state-value\n",
        "\n",
        "                    # # Original\n",
        "                    # self.vf_loss = mse(tf.slice(tf.squeeze(train_model.value_flat), [0], [tf.shape(self.rewards_ph)[0]]), self.rewards_ph) # Cut-off source samples\n",
        "                    # loss = self.pg_loss - self.entropy * self.ent_coef + self.vf_loss * self.vf_coef\n",
        "\n",
        "                    # Dueling network\n",
        "                    if isinstance(self.action_space, Discrete): # discrete action space\n",
        "                        q_estimate = train_model.value_flat + tf.reduce_sum(train_model.q_value * tf.one_hot(self.actions_ph, self.action_space.n), axis = 1) \\\n",
        "                                     - tf.reduce_mean(train_model.q_value, axis=-1)\n",
        "                    else: # continuous\n",
        "                        pass\n",
        "                    self.vf_loss = mse(tf.slice(tf.squeeze(q_estimate), [0], [tf.shape(self.rewards_ph)[0]]), self.rewards_ph) # Estimate (state,action)-value\n",
        "                    # https://arxiv.org/pdf/1708.04782.pdf#page=9, https://arxiv.org/pdf/1602.01783.pdf#page=4\n",
        "                    # and https://github.com/dennybritz/reinforcement-learning/issues/34\n",
        "                    # suggest to add an entropy component in order to improve exploration.\n",
        "                    loss = self.pg_loss - self.entropy * self.ent_coef + self.vf_loss * self.vf_coef # Original\n",
        "\n",
        "                    # To check unbiased PG\n",
        "                    params_test = tf_util.get_trainable_vars(\"model\")\n",
        "                    pg_loss_trg = tf.reduce_mean(tf.slice(self.advs_ph * neglogpac, [0], [tf.shape(self.rewards_ph)[0]]))\n",
        "                    pg_loss_src = tf.reduce_mean(tf.slice(self.advs_ph * neglogpac, [tf.shape(self.rewards_ph)[0]], [tf.shape(self.advs_ph)[0] - tf.shape(self.rewards_ph)[0]]))\n",
        "\n",
        "                    # pg_loss_trg = tf.reduce_mean(tf.slice(self.advs_ph * neglogpac, [0], [tf.shape(self.advs_ph)[0]/2]))\n",
        "                    # pg_loss_src = tf.reduce_mean(tf.slice(self.advs_ph * neglogpac, [10], [tf.shape(self.advs_ph)[0]/2]))\n",
        "\n",
        "                    pg_grads_trg = tf.gradients(pg_loss_trg, params_test)\n",
        "                    pg_grads_src = tf.gradients(pg_loss_src, params_test)\n",
        "                    vf_grads = tf.gradients(self.vf_loss, params_test)\n",
        "\n",
        "                    _, pg_grads_trg = tf.clip_by_global_norm(pg_grads_trg, 0.5)\n",
        "                    _, pg_grads_src = tf.clip_by_global_norm(pg_grads_src, 0.5)\n",
        "                    _, vf_grads = tf.clip_by_global_norm(vf_grads, 0.5)\n",
        "\n",
        "                    with tf.variable_scope(\"grad_info\", reuse=False):\n",
        "                        tf.summary.scalar('pg_loss_trg', tf.reduce_mean(pg_loss_trg))\n",
        "                        tf.summary.scalar('pg_loss_src', tf.reduce_mean(pg_loss_src))\n",
        "                        tf.summary.scalar('pg_grads_trg', tf.reduce_mean(pg_grads_trg))\n",
        "                        tf.summary.scalar('pg_grads_src', tf.reduce_mean(pg_grads_src))\n",
        "                        tf.summary.scalar('vf_grads', tf.reduce_mean(vf_grads))\n",
        "\n",
        "                    tf.summary.scalar('entropy_loss', self.entropy)\n",
        "                    tf.summary.scalar('policy_gradient_loss', self.pg_loss)\n",
        "                    tf.summary.scalar('value_function_loss', self.vf_loss)\n",
        "\n",
        "                    # Original\n",
        "                    tf.summary.scalar('loss', loss)\n",
        "                    self.params = tf_util.get_trainable_vars(\"model\")\n",
        "                    grads = tf.gradients(loss, self.params)\n",
        "                    if self.max_grad_norm is not None:\n",
        "                        grads, _ = tf.clip_by_global_norm(grads, self.max_grad_norm)\n",
        "                    grads = list(zip(grads, self.params))\n",
        "\n",
        "                    # # Update policy\n",
        "                    # tf.summary.scalar('loss_pg', loss_p)\n",
        "                    # self.params = tf_util.get_trainable_vars(\"model\")\n",
        "                    # grads_p = tf.gradients(loss_p, self.params)\n",
        "                    # if self.max_grad_norm is not None:\n",
        "                    #     grads_p, _ = tf.clip_by_global_norm(grads_p, self.max_grad_norm)\n",
        "                    # grads_p = list(zip(grads_p, self.params))\n",
        "                    #\n",
        "                    # # Update value\n",
        "                    # tf.summary.scalar('loss_vf', loss_v)\n",
        "                    # grads_v = tf.gradients(loss_v, self.params)\n",
        "                    # if self.max_grad_norm is not None:\n",
        "                    #     grads_v, _ = tf.clip_by_global_norm(grads_v, self.max_grad_norm)\n",
        "                    # grads_v = list(zip(grads_v, self.params))\n",
        "\n",
        "                with tf.variable_scope(\"input_info\", reuse=False):\n",
        "                    tf.summary.scalar('discounted_rewards', tf.reduce_mean(self.rewards_ph))\n",
        "                    tf.summary.scalar('learning_rate', tf.reduce_mean(self.learning_rate_ph))\n",
        "                    tf.summary.scalar('advantage', tf.reduce_mean(self.advs_ph))\n",
        "                    if self.full_tensorboard_log:\n",
        "                        tf.summary.histogram('discounted_rewards', self.rewards_ph)\n",
        "                        tf.summary.histogram('learning_rate', self.learning_rate_ph)\n",
        "                        tf.summary.histogram('advantage', self.advs_ph)\n",
        "                        if tf_util.is_image(self.observation_space):\n",
        "                            tf.summary.image('observation', train_model.obs_ph)\n",
        "                        else:\n",
        "                            tf.summary.histogram('observation', train_model.obs_ph)\n",
        "\n",
        "                trainer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=self.learning_rate_ph, decay=self.alpha,\n",
        "                                                    epsilon=self.epsilon)\n",
        "                self.apply_backprop = trainer.apply_gradients(grads) # Original\n",
        "                # self.apply_backprop_p = trainer.apply_gradients(grads_p) # policy update\n",
        "                # self.apply_backprop_v = trainer.apply_gradients(grads_v) # value update\n",
        "\n",
        "                self.train_model = train_model\n",
        "                self.step_model = step_model\n",
        "                if self.source_env is not None:\n",
        "                    self.classifier = classifier\n",
        "                self.step = step_model.step\n",
        "                self.proba_step = step_model.proba_step\n",
        "                self.value = step_model.value\n",
        "                self.initial_state = step_model.initial_state\n",
        "                tf.global_variables_initializer().run(session=self.sess)\n",
        "\n",
        "                self.summary = tf.summary.merge_all()\n",
        "\n",
        "    def _train_step(self, obs, states, rewards, masks, actions, values, q_values, n_sources, update, writer=None):\n",
        "        \"\"\"\n",
        "        applies a training step to the model\n",
        "\n",
        "        :param obs: ([float]) The input observations\n",
        "        :param states: ([float]) The states (used for recurrent policies)\n",
        "        :param rewards: ([float]) The rewards from the environment\n",
        "        :param masks: ([bool]) Whether or not the episode is over (used for recurrent policies)\n",
        "        :param actions: ([float]) The actions taken\n",
        "        :param values: ([float]) The logits values\n",
        "        :param update: (int) the current step iteration\n",
        "        :param writer: (TensorFlow Summary.writer) the writer for tensorboard\n",
        "        :return: (float, float, float) policy loss, value loss, policy entropy\n",
        "        \"\"\"\n",
        "        # State-value based\n",
        "        # advs = rewards - values # with baseline\n",
        "        # advs = rewards # without baseline\n",
        "        advs = q_values\n",
        "\n",
        "        # (state,action)-value based\n",
        "        # advs = q_values - values\n",
        "        # advs = q_values\n",
        "\n",
        "        cur_lr = None\n",
        "        for _ in range(n_sources):\n",
        "            cur_lr = self.learning_rate_schedule.value()\n",
        "        assert cur_lr is not None, \"Error: the observation input array cannon be empty\"\n",
        "\n",
        "        td_map = {self.train_model.obs_ph: obs, self.actions_ph: actions, self.advs_ph: advs,\n",
        "                  self.rewards_ph: rewards[0:n_sources], self.learning_rate_ph: cur_lr}\n",
        "        if states is not None:\n",
        "            td_map[self.train_model.states_ph] = states\n",
        "            td_map[self.train_model.dones_ph] = masks\n",
        "\n",
        "        if writer is not None:\n",
        "            # run loss backprop with summary, but once every 10 runs save the metadata (memory, compute time, ...)\n",
        "            if self.full_tensorboard_log and (1 + update) % 10 == 0:\n",
        "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "                run_metadata = tf.RunMetadata()\n",
        "                summary, policy_loss, value_loss, policy_entropy, _ = self.sess.run(\n",
        "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.apply_backprop],\n",
        "                    td_map, options=run_options, run_metadata=run_metadata) # test_purpose to track gradient values\n",
        "                writer.add_run_metadata(run_metadata, 'step%d' % (update * self.n_batch))\n",
        "            else:\n",
        "                summary, policy_loss, value_loss, policy_entropy, _ = self.sess.run(\n",
        "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.apply_backprop], td_map)  # test_purpose to track gradient values\n",
        "            writer.add_summary(summary, update * self.n_batch)\n",
        "\n",
        "        else:\n",
        "            policy_loss, value_loss, policy_entropy, _ = self.sess.run(\n",
        "                [self.pg_loss, self.vf_loss, self.entropy, self.apply_backprop], td_map)\n",
        "\n",
        "        return policy_loss, value_loss, policy_entropy\n",
        "\n",
        "    def predict_customize(self, observation, state=None, mask=None, deterministic=False):\n",
        "        if state is None:\n",
        "            state = self.initial_state\n",
        "        if mask is None:\n",
        "            mask = [False for _ in range(self.n_envs)]\n",
        "        observation = np.array(observation)\n",
        "        vectorized_env = self._is_vectorized_observation(observation, self.observation_space)\n",
        "\n",
        "        observation = observation.reshape((-1,) + self.observation_space.shape)\n",
        "        actions, _, states, _, _ = self.step(observation, state, mask, deterministic=deterministic)\n",
        "\n",
        "        clipped_actions = actions\n",
        "        # Clip the actions to avoid out of bound error\n",
        "        if isinstance(self.action_space, gym.spaces.Box):\n",
        "            clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
        "\n",
        "        if not vectorized_env:\n",
        "            if state is not None:\n",
        "                raise ValueError(\"Error: The environment must be vectorized when using recurrent policies.\")\n",
        "            clipped_actions = clipped_actions[0]\n",
        "\n",
        "        return clipped_actions, states"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E0L_IPNBTSg",
        "outputId": "e976e7e8-1eae-497f-d8cd-6b75fb2ab27a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "model = A2C_IS(policy_IS, target_env, source_env, source_demo=None, ent_coef=0.0, verbose=1, tensorboard_log=\"./test_for_metric/\", seed=0, n_cpu_tf_sess=1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrapping the env in a DummyVecEnv.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:121: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2slIdv8qCyLJ"
      },
      "source": [
        "A2C_Runner 코드 일부로 샘플링 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPMEUK72Denj",
        "outputId": "fe5dc63c-2a40-4678-a1c9-06a873df1341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#obs = source_env.reset()\n",
        "obs = np.array([[0.1,0.1,0.1,0.1]])\n",
        "#actions, values, states, _, q_values = self.model.step(self.obs, self.states, self.dones) # Get action, values from policy model\n",
        "actions, values, states, _, q_values = model.step(obs, True) # Get action, values from policy model\n",
        "clipped_actions = actions\n",
        "print (obs, clipped_actions)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.1 0.1 0.1 0.1]] [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRS3I8THIag6",
        "outputId": "d55cb7e6-7478-469a-85f3-2d5893947931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "# Source Transition\n",
        "for i in range(10) :\n",
        "  #obs, rewards, dones, infos = self.env.step(clipped_actions)\n",
        "  new_obs, rewards, dones, infos = source_env.step2(obs, clipped_actions)\n",
        "  print (i, new_obs)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [[  0.102    -19.399399   0.102     29.232325]]\n",
            "1 [[  0.102    -19.399399   0.102     29.232325]]\n",
            "2 [[  0.102    -19.399399   0.102     29.232325]]\n",
            "3 [[  0.102    -19.399399   0.102     29.232325]]\n",
            "4 [[  0.102    -19.399399   0.102     29.232325]]\n",
            "5 [[  0.102    -19.399399   0.102     29.232325]]\n",
            "6 [[  0.102    -19.399399   0.102     29.232325]]\n",
            "7 [[  0.102    -19.399399   0.102     29.232325]]\n",
            "8 [[  0.102    -19.399399   0.102     29.232325]]\n",
            "9 [[  0.102    -19.399399   0.102     29.232325]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcGSfBcJJju3",
        "outputId": "008d5f58-9eb5-40ff-eb92-3c444e2130b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "# Target Transition\n",
        "for i in range(10) :\n",
        "  #obs, rewards, dones, infos = self.env.step(clipped_actions)\n",
        "  new_obs, rewards, dones, infos = target_env.step2(obs, clipped_actions)\n",
        "  print (i, new_obs)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [[ 0.102      -0.09640235  0.102       0.42248276]]\n",
            "1 [[ 0.102      -0.09640235  0.102       0.42248276]]\n",
            "2 [[ 0.102      -0.09640235  0.102       0.42248276]]\n",
            "3 [[ 0.102      -0.09640235  0.102       0.42248276]]\n",
            "4 [[ 0.102      -0.09640235  0.102       0.42248276]]\n",
            "5 [[ 0.102      -0.09640235  0.102       0.42248276]]\n",
            "6 [[ 0.102      -0.09640235  0.102       0.42248276]]\n",
            "7 [[ 0.102      -0.09640235  0.102       0.42248276]]\n",
            "8 [[ 0.102      -0.09640235  0.102       0.42248276]]\n",
            "9 [[ 0.102      -0.09640235  0.102       0.42248276]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCOiMT1kJsPB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}