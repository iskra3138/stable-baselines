{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stable-baselines_2_gym_wrappers_saving_loading.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/stable-baselines/blob/main/stable_baselines_2_gym_wrappers_saving_loading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ezJ3Y7XRUnj"
      },
      "source": [
        "# Stable Baselines Tutorial - Gym wrappers, saving and loading models\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19\n",
        "\n",
        "Stable-Baselines: https://github.com/hill-a/stable-baselines\n",
        "\n",
        "Documentation: https://stable-baselines.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines zoo: https://github.com/araffin/rl-baselines-zoo\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn how to use *Gym Wrappers* which allow to do monitoring, normalization, limit the number of steps, feature augmentation, ...\n",
        "- 이번 노트북을 토해 *GymWrappers*를 사용하는 방법을 배움\n",
        "  - 이를 통해 모니터링, normalization, 스텝수 제한, Feature augmentation등을 할 수 있음\n",
        "\n",
        "\n",
        "You will also see the *loading* and *saving* functions, and how to read the outputed files for possible exporting.\n",
        "- 모델을 저장하고 읽는 것도 해볼 것임\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFdlFByORUnl",
        "outputId": "64d51d32-c623-46b8-9142-eacf49642566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt install swig\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "swig is already the newest version (3.0.12-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "Requirement already satisfied: stable-baselines[mpi]==2.10.0 in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.18.5)\n",
            "Requirement already satisfied: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grXe85G9RUnp"
      },
      "source": [
        "import gym\n",
        "from stable_baselines import A2C, SAC, PPO2, TD3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMPAn1SRd32f"
      },
      "source": [
        "# Saving and loading\n",
        "\n",
        "Saving and loading stable-baselines models is straightforward: you can directly call `.save()` and `.load()` on the models.\n",
        "- stable-baselines 모델을 저장하고 불러오는 것은 매우 직관적임\n",
        "  - .save(), .load()를 바로 호출함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBNFnN4Gd32g",
        "outputId": "ae03c1bf-0453-4f6c-8122-e4cd8e366ace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Create save dir\n",
        "save_dir = \"/tmp/gym/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model = PPO2('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n",
        "# The model will be saved under PPO2_tutorial.zip\n",
        "model.save(save_dir + \"/PPO2_tutorial\")\n",
        "\n",
        "# sample an observation from the environment\n",
        "obs = model.env.observation_space.sample()\n",
        "\n",
        "# Check prediction before saving\n",
        "print(\"pre saved\", model.predict(obs, deterministic=True))\n",
        "\n",
        "del model # delete trained model to demonstrate loading\n",
        "\n",
        "loaded_model = PPO2.load(save_dir + \"/PPO2_tutorial\")\n",
        "# Check that the prediction is the same after loading (for the same observation)\n",
        "print(\"loaded\", loaded_model.predict(obs, deterministic=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pre saved (array([-0.19883569], dtype=float32), None)\n",
            "loaded (array([-0.19883569], dtype=float32), None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXWPrVqId32o"
      },
      "source": [
        "Saving in stable-baselines is quite powerful, as you save the training hyperparameters, with the current weights. This means in practice, you can simply load a custom model, without redefining the parameters, and continue learning.\n",
        "- stable-baselines에서 저장은 학습 HP, Weight를 모두 함께 저장하므로 굉장히 powerful.\n",
        "- 모델을 불러올 때, 파라미터를 다시 재정의할 필요없이 계속해서 재학습이 가능해짐\n",
        "\n",
        "\n",
        "The loading function can also update the model's class variables when loading.\n",
        "- loading function은 로딩 시 모델 클래스의 변수들을 업데이트할 수도 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCtxrAbXd32q",
        "outputId": "4393ff3c-16a0-4f6c-db4e-df35fe709639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        }
      },
      "source": [
        "import os\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Create save dir\n",
        "save_dir = \"/tmp/gym/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model = A2C('MlpPolicy', 'Pendulum-v0', verbose=0, gamma=0.9, n_steps=20).learn(8000)\n",
        "# The model will be saved under A2C_tutorial.zip\n",
        "model.save(save_dir + \"/A2C_tutorial\")\n",
        "\n",
        "del model # delete trained model to demonstrate loading\n",
        "\n",
        "# load the model, and when loading set verbose to 1\n",
        "loaded_model = A2C.load(save_dir + \"/A2C_tutorial\", verbose=1)\n",
        "\n",
        "# show the save hyperparameters\n",
        "print(\"loaded:\", \"gamma =\", loaded_model.gamma, \"n_steps =\", loaded_model.n_steps)\n",
        "\n",
        "# as the environment is not serializable, we need to set a new instance of the environment\n",
        "# environment는 저장되지 않기 때문에, 새로운 environment instance를 설정해줘야 함\n",
        "loaded_model.set_env(DummyVecEnv([lambda: gym.make('Pendulum-v0')]))\n",
        "# and continue training\n",
        "loaded_model.learn(8000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "loaded: gamma = 0.9 n_steps = 20\n",
            "---------------------------------\n",
            "| explained_variance | 0.00487  |\n",
            "| fps                | 90       |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 1.42     |\n",
            "| total_timesteps    | 20       |\n",
            "| value_loss         | 2.56e+03 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.003    |\n",
            "| fps                | 893      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 1.42     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 2.46e+03 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0093   |\n",
            "| fps                | 948      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 1.42     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 2.19e+03 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00182  |\n",
            "| fps                | 949      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 1.42     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 1.91e+03 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0452  |\n",
            "| fps                | 961      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 1.43     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 1.72e+03 |\n",
            "---------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines.a2c.a2c.A2C at 0x7f2f317d3240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKwupU-Jgxjm"
      },
      "source": [
        "# Gym and VecEnv wrappers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds4AAfmISQIA"
      },
      "source": [
        "## Anatomy of a gym wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnTS9e9hTzZZ"
      },
      "source": [
        "A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n",
        "\n",
        "Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n",
        "There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://github.com/openai/gym/tree/master/gym/wrappers)\n",
        "\n",
        "- gym wrapper는 gym 인터페이스를 따름\n",
        "  - reset (), step () 메서드를 가짐.\n",
        "\n",
        "- wrapper는 environement *주변에* 있기 때문에, self.env로 액세스 할 수 있음\n",
        "  - 원래 environment를 수정하지 않고도 쉽게 상호 작용할 수 있음\n",
        "- 사전 정의 된 많은 wrapper들이 있음. 전체 목록은 gym 문서를 참조."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYo0C0TQSL3c"
      },
      "source": [
        "class CustomWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(CustomWrapper, self).__init__(env)\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    obs = self.env.reset()\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    obs, reward, done, info = self.env.step(action)\n",
        "    return obs, reward, done, info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zeGuyICUN26"
      },
      "source": [
        "## First example: limit the episode length\n",
        "첫번째 예제: 에피소드 길이 제한\n",
        "\n",
        "One practical use case of a wrapper is when you want to limit the number of steps by episode, for that you will need to overwrite the `done` signal when the limit is reached. It is also a good practice to pass that information in the `info` dictionnary.\n",
        "- wrapper의 실제 use case 중 하나는 에피소드별로 step 수를 제한하려는 경우\n",
        "  - 제한에 도달하면 'done'신호를 덮어 써야할 필요가 있음\n",
        "  - 'info' dictionary에 해당 정보를 전달하는 것도 좋은 방법\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb2U4_K6SNUx"
      },
      "source": [
        "class TimeLimitWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  :param max_steps: (int) Max number of steps per episode\n",
        "  \"\"\"\n",
        "  def __init__(self, env, max_steps=100):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(TimeLimitWrapper, self).__init__(env)\n",
        "    self.max_steps = max_steps\n",
        "    # Counter of steps per episode\n",
        "    self.current_step = 0\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    # Reset the counter\n",
        "    self.current_step = 0\n",
        "    return self.env.reset()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    self.current_step += 1\n",
        "    obs, reward, done, info = self.env.step(action)\n",
        "    # Overwrite the done signal when \n",
        "    if self.current_step >= self.max_steps:\n",
        "      done = True\n",
        "      # Update the info dict to signal that the limit was exceeded\n",
        "      info['time_limit_reached'] = True\n",
        "    return obs, reward, done, info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZufaUJwVM9w"
      },
      "source": [
        "#### Test the wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szZ43D5PVB07"
      },
      "source": [
        "from gym.envs.classic_control.pendulum import PendulumEnv\n",
        "\n",
        "# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n",
        "env = PendulumEnv()\n",
        "# Wrap the environment\n",
        "env = TimeLimitWrapper(env, max_steps=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cencka9iVg9V",
        "outputId": "2d4b1139-0d76-498b-f749-4cea676de203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "n_steps = 0\n",
        "while not done:\n",
        "  # Take random actions\n",
        "  random_action = env.action_space.sample()\n",
        "  obs, reward, done, info = env.step(random_action)\n",
        "  n_steps += 1\n",
        "\n",
        "print(n_steps, info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 {'time_limit_reached': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkMYA63sV9aA"
      },
      "source": [
        "In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIIJbSyQW9R-"
      },
      "source": [
        "## Second example: normalize actions\n",
        "두번째 예제 : action 노멀라이즈\n",
        "\n",
        "It is usually a good idea to normalize observations and actions before giving it to the agent, this prevent [hard to debug issue](https://github.com/hill-a/stable-baselines/issues/473).\n",
        "-일반적으로 관찰 및 조치를 에이전트에 제공하기 전에 정규화하는 것이 좋음\n",
        "  - 이를 통해 디버그하기 어려운 문제를 방지 할 수 있음\n",
        "\n",
        "In this example, we are going to normalize the action space of *Pendulum-v0* so it lies in [-1, 1] instead of [-2, 2].\n",
        "- 이 예에서는 Pendulum-v0의 작업 공간을 정규화하여 [-2, 2] 대신 [-1, 1]이 되게 함\n",
        "\n",
        "Note: here we are dealing with continuous actions, hence the `gym.Box` space\n",
        "- 참고 : 여기서 우리는 연속적인 동작을 다루고 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5E6kZfzW8vy"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class NormalizeActionWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    # Retrieve the action space\n",
        "    action_space = env.action_space\n",
        "    assert isinstance(action_space, gym.spaces.Box), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
        "    # Retrieve the max/min values\n",
        "    self.low, self.high = action_space.low, action_space.high\n",
        "\n",
        "    # We modify the action space, so all actions will lie in [-1, 1]\n",
        "    env.action_space = gym.spaces.Box(low=-1, high=1, shape=action_space.shape, dtype=np.float32)\n",
        "\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(NormalizeActionWrapper, self).__init__(env)\n",
        "  \n",
        "  def rescale_action(self, scaled_action):\n",
        "      \"\"\"\n",
        "      Rescale the action from [-1, 1] to [low, high]\n",
        "      (no need for symmetric action space)\n",
        "      :param scaled_action: (np.ndarray)\n",
        "      :return: (np.ndarray)\n",
        "      \"\"\"\n",
        "      return self.low + (0.5 * (scaled_action + 1.0) * (self.high -  self.low))\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    # Reset the counter\n",
        "    return self.env.reset()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    # Rescale action from [-1, 1] to original [low, high] interval\n",
        "    rescaled_action = self.rescale_action(action)\n",
        "    obs, reward, done, info = self.env.step(rescaled_action)\n",
        "    return obs, reward, done, info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmJ0eahNaR6K"
      },
      "source": [
        "#### Test before rescaling actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEnjBwisaQIx",
        "outputId": "6f332723-bda3-4849-dab4-88af5343c41b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "original_env = gym.make(\"Pendulum-v0\")\n",
        "\n",
        "print(original_env.action_space.low)\n",
        "for _ in range(10):\n",
        "  print(original_env.action_space.sample())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.]\n",
            "[1.1236833]\n",
            "[0.8284715]\n",
            "[1.1950923]\n",
            "[0.38227656]\n",
            "[0.91574895]\n",
            "[-0.42847648]\n",
            "[1.580651]\n",
            "[1.3893884]\n",
            "[-1.5455852]\n",
            "[1.3891894]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvcll2L3afVd"
      },
      "source": [
        "#### Test the NormalizeAction wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsCM9AUGaeBN",
        "outputId": "37c2fc8d-cec0-46dd-f105-e17f2fd8552d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "env = NormalizeActionWrapper(gym.make(\"Pendulum-v0\"))\n",
        "\n",
        "print(env.action_space.low)\n",
        "\n",
        "for _ in range(10):\n",
        "  print(env.action_space.sample())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.]\n",
            "[0.31806344]\n",
            "[-0.9056247]\n",
            "[-0.7164699]\n",
            "[0.43610492]\n",
            "[0.31373236]\n",
            "[0.5897452]\n",
            "[0.8344767]\n",
            "[-0.5719794]\n",
            "[-0.44559914]\n",
            "[-0.84648055]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5h5kk2mbGNs"
      },
      "source": [
        "#### Test with a RL algorithm\n",
        "\n",
        "We are going to use the Monitor wrapper of stable baselines, wich allow to monitor training stats (mean episode reward, mean episode length)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9FNCN8ybOVU"
      },
      "source": [
        "from stable_baselines.bench import Monitor\n",
        "from stable_baselines.common.vec_env import DummyVecEnv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wutM3c1GbfGP"
      },
      "source": [
        "env = Monitor(gym.make('Pendulum-v0'), filename=None, allow_early_resets=True)\n",
        "env = DummyVecEnv([lambda: env])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cxnE5bdaQ_3",
        "outputId": "af87c65f-f6da-4b7a-a94f-a42c62c343ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "model = A2C(\"MlpPolicy\", env, verbose=1).learn(int(1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| explained_variance | 0.00532  |\n",
            "| fps                | 24       |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 1.42     |\n",
            "| total_timesteps    | 5        |\n",
            "| value_loss         | 501      |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 200       |\n",
            "| ep_reward_mean     | -1.32e+03 |\n",
            "| explained_variance | -0.0102   |\n",
            "| fps                | 586       |\n",
            "| nupdates           | 100       |\n",
            "| policy_entropy     | 1.42      |\n",
            "| total_timesteps    | 500       |\n",
            "| value_loss         | 759       |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 200       |\n",
            "| ep_reward_mean     | -1.08e+03 |\n",
            "| explained_variance | 0.0809    |\n",
            "| fps                | 680       |\n",
            "| nupdates           | 200       |\n",
            "| policy_entropy     | 1.42      |\n",
            "| total_timesteps    | 1000      |\n",
            "| value_loss         | 35.9      |\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJFSM-Drb3Wc"
      },
      "source": [
        "With the action wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GszFZthob2wM"
      },
      "source": [
        "normalized_env = Monitor(gym.make('Pendulum-v0'), filename=None, allow_early_resets=True)\n",
        "# Note that we can use multiple wrappers\n",
        "normalized_env = NormalizeActionWrapper(normalized_env)\n",
        "normalized_env = DummyVecEnv([lambda: normalized_env])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrKJEO4NcIMd",
        "outputId": "fa3b1ba2-8617-4388-f2c2-5056687b346c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "model_2 = A2C(\"MlpPolicy\", normalized_env, verbose=1).learn(int(1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| explained_variance | -0.0237  |\n",
            "| fps                | 26       |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 1.42     |\n",
            "| total_timesteps    | 5        |\n",
            "| value_loss         | 166      |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 200       |\n",
            "| ep_reward_mean     | -1.04e+03 |\n",
            "| explained_variance | -0.00917  |\n",
            "| fps                | 594       |\n",
            "| nupdates           | 100       |\n",
            "| policy_entropy     | 1.42      |\n",
            "| total_timesteps    | 500       |\n",
            "| value_loss         | 470       |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 200       |\n",
            "| ep_reward_mean     | -1.06e+03 |\n",
            "| explained_variance | 0.00124   |\n",
            "| fps                | 653       |\n",
            "| nupdates           | 200       |\n",
            "| policy_entropy     | 1.42      |\n",
            "| total_timesteps    | 1000      |\n",
            "| value_loss         | 1.18e+03  |\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BxqXd_6dpJx"
      },
      "source": [
        "## Additional wrappers: VecEnvWrappers\n",
        "\n",
        "In the same vein as gym wrappers, stable baselines provide wrappers for `VecEnv`. Among the different that exist (and you can create your own), you should know: \n",
        "\n",
        "- VecNormalize: it computes a running mean and standard deviation to normalize observation and returns\n",
        "- VecFrameStack: it stacks several consecutive observations (useful to integrate time in the observation, e.g. sucessive frame of an atari game)\n",
        "\n",
        "More info in the [documentation](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#wrappers)\n",
        "\n",
        "Note: when using `VecNormalize` wrapper, you must save the running mean and std along with the model, otherwise you will not get proper results when loading the agent again. If you use the [rl zoo](https://github.com/araffin/rl-baselines-zoo), this is done automatically\n",
        "\n",
        "- gym wrappers와 같은 맥락에서 stable baselines은 'VecEnv'에 대한 wrapper를 제공\n",
        "존재하는 다른 것 (그리고 직접 만들 수 있음) 중에서 다음을 알아야 함\n",
        "  - VecNormalize : 관측을 정규화하기 위해 실행 평균과 표준 편차를 계산하고 반환\n",
        "  - VecFrameStack : 여러 개의 연속된 observation을 쌓음 \n",
        "    - observation에 시간을 통합하는 데 유용함 (예 : 아타리 게임의 연속 프레임)\n",
        "\n",
        "\n",
        "참고 : 'VecNormalize' wrapper를 사용하는 경우, 실행 중인 평균 및 표준편차를 모델과 함께 저장해야 함. 그렇지 않으면 에이전트를 다시 로드 할 때 적절한 결과를 얻지 못함.\n",
        " - rl zoo를 사용하면 자동으로 수행됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuIcbfv3g9dd"
      },
      "source": [
        "from stable_baselines.common.vec_env import VecNormalize, VecFrameStack\n",
        "\n",
        "env = DummyVecEnv([lambda: gym.make(\"Pendulum-v0\")])\n",
        "normalized_vec_env = VecNormalize(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PAbu21pg90A",
        "outputId": "36876f54-3e51-4bdb-c795-ff3421e4328f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "obs = normalized_vec_env.reset()\n",
        "for _ in range(10):\n",
        "  action = [normalized_vec_env.action_space.sample()]\n",
        "  obs, reward, _, _ = normalized_vec_env.step(action)\n",
        "  print(obs, reward)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.0069937  -0.00201678 -0.00252103]] [-1.8756421]\n",
            "[[-0.30897885 -0.89460939 -0.99908061]] [-1.3867719]\n",
            "[[-0.85137728 -1.25493211 -1.1367014 ]] [-1.3548794]\n",
            "[[-1.38664769 -1.48970432 -1.46118034]] [-1.2629999]\n",
            "[[-1.6032098  -1.55958178 -1.20179075]] [-1.3572179]\n",
            "[[-1.79896311 -1.66288467 -1.52662146]] [-1.0916774]\n",
            "[[-1.93717621 -1.73059521 -1.5940493 ]] [-1.1513095]\n",
            "[[-2.06685232 -1.78746985 -1.74711613]] [-1.121458]\n",
            "[[-2.17348952 -1.8130202  -1.81983126]] [-1.1469169]\n",
            "[[-2.25909019 -1.79650298 -1.87612573]] [-1.1441946]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEpTys28Wz05"
      },
      "source": [
        "## Exercise: code you own monitor wrapper\n",
        "실습: 너의 monitor wrapper를 만들어라\n",
        "\n",
        "Now that you know how does a wrapper work and what you can do with it, it's time to experiment.\n",
        "- wrapper가 어떻게 작동하는 지와 그것으로 무엇을 할 수 있는 지를 알게 되었으니, 실습해볼 것\n",
        "\n",
        "The goal here is to create a wrapper that will monitor the training progress, storing both the episode reward (sum of reward for one episode) and episode length (number of steps in for the last episode).\n",
        "- training progress를 모니터링하고, episode reward와 length를 저장하는 wrapper 정의\n",
        "\n",
        "You will return those values using the `info` dict after each end of episode.\n",
        "- episode의 매 마지막에 'info' dict를 사용하여 해당 값들을 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FWeDRd5W7hO"
      },
      "source": [
        "class MyMonitorWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(MyMonitorWrapper, self).__init__(env)\n",
        "    # === YOUR CODE HERE ===#\n",
        "    # Initialize the variables that will be used\n",
        "    # to store the episode length and episode reward\n",
        "\n",
        "    # ====================== #\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    obs = self.env.reset()\n",
        "    # === YOUR CODE HERE ===#\n",
        "    # Reset the variables\n",
        "\n",
        "    # ====================== #\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    obs, reward, done, info = self.env.step(action)\n",
        "    # === YOUR CODE HERE ===#\n",
        "    # Update the current episode reward and episode length\n",
        "\n",
        "    # ====================== #\n",
        "\n",
        "    if done:\n",
        "      # === YOUR CODE HERE ===#\n",
        "      # Store the episode length and episode reward in the info dict\n",
        "\n",
        "      # ====================== #\n",
        "      return obs, reward, done, info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4fY4QwWXNFK"
      },
      "source": [
        "#### Test your wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJbUG-A_liYt",
        "outputId": "1409604a-eb4a-4345-c15b-02d51927c9d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "# To use LunarLander, you need to install box2d box2d-kengz (pip) and swig (apt-get)\n",
        "!pip install box2d box2d-kengz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/0b/d48d42dd9e19ce83a3fb4eee074e785b6c6ea612a2244dc2ef69427d338b/Box2D-2.3.10-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 5.9MB/s \n",
            "\u001b[?25hCollecting box2d-kengz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/20/51d6c0c87f7642efb709c518fb0ca8e5eab068259588552c41da5926ae27/Box2D-kengz-2.3.3.tar.gz (425kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 44.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-kengz\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp36-cp36m-linux_x86_64.whl size=2014815 sha256=72e1ac9548cf46cf3219e48e8e282a3905034c4570fe48e60606e483c88b3d22\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/ae/e5/8bc678d262caad94659c199c540550e59d03dd3bd3684d4f1a\n",
            "Successfully built box2d-kengz\n",
            "Installing collected packages: box2d, box2d-kengz\n",
            "Successfully installed box2d-2.3.10 box2d-kengz-2.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWZp1olSXMUg"
      },
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "# === YOUR CODE HERE ===#\n",
        "# Wrap the environment\n",
        "\n",
        "# Reset the environment\n",
        "\n",
        "# Take random actions in the enviromnent and check\n",
        "# that it returns the correct values after the end of each episode\n",
        "\n",
        "# ====================== #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2IqSM2eOt8"
      },
      "source": [
        " # Conclusion\n",
        " \n",
        " In this notebook, we have seen:\n",
        " - how to easily save and load a model\n",
        " - what is wrapper and what we can do with it\n",
        " - how to create your own wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhWB_bHpSkas"
      },
      "source": [
        "## Wrapper Bonus: changing the observation space: a wrapper for episode of fixed length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBlS9YxYSpJn"
      },
      "source": [
        "from gym.wrappers import TimeLimit\n",
        "\n",
        "class TimeFeatureWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Add remaining time to observation space for fixed length episodes.\n",
        "    See https://arxiv.org/abs/1712.00378 and https://github.com/aravindr93/mjrl/issues/13.\n",
        "\n",
        "    :param env: (gym.Env)\n",
        "    :param max_steps: (int) Max number of steps of an episode\n",
        "        if it is not wrapped in a TimeLimit object.\n",
        "    :param test_mode: (bool) In test mode, the time feature is constant,\n",
        "        equal to zero. This allow to check that the agent did not overfit this feature,\n",
        "        learning a deterministic pre-defined sequence of actions.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, max_steps=1000, test_mode=False):\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
        "        # Add a time feature to the observation\n",
        "        low, high = env.observation_space.low, env.observation_space.high\n",
        "        low, high= np.concatenate((low, [0])), np.concatenate((high, [1.]))\n",
        "        env.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
        "\n",
        "        super(TimeFeatureWrapper, self).__init__(env)\n",
        "\n",
        "        if isinstance(env, TimeLimit):\n",
        "            self._max_steps = env._max_episode_steps\n",
        "        else:\n",
        "            self._max_steps = max_steps\n",
        "        self._current_step = 0\n",
        "        self._test_mode = test_mode\n",
        "\n",
        "    def reset(self):\n",
        "        self._current_step = 0\n",
        "        return self._get_obs(self.env.reset())\n",
        "\n",
        "    def step(self, action):\n",
        "        self._current_step += 1\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        return self._get_obs(obs), reward, done, info\n",
        "\n",
        "    def _get_obs(self, obs):\n",
        "        \"\"\"\n",
        "        Concatenate the time feature to the current observation.\n",
        "\n",
        "        :param obs: (np.ndarray)\n",
        "        :return: (np.ndarray)\n",
        "        \"\"\"\n",
        "        # Remaining time is more general\n",
        "        time_feature = 1 - (self._current_step / self._max_steps)\n",
        "        if self._test_mode:\n",
        "            time_feature = 1.0\n",
        "        # Optionnaly: concatenate [time_feature, time_feature ** 2]\n",
        "        return np.concatenate((obs, [time_feature]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-vWgkZzd4F1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojn4nvNNRUoT"
      },
      "source": [
        "## Going further - Saving format \n",
        "\n",
        "The format for saving and loading models has been recently revamped as of Stable-Baselines (>2.7.0).\n",
        "\n",
        "It is a zip-archived JSON dump and NumPy zip archive of the arrays:\n",
        "```\n",
        "saved_model.zip/\n",
        "├── data              JSON file of class-parameters (dictionary)\n",
        "├── parameter_list    JSON file of model parameters and their ordering (list)\n",
        "├── parameters        Bytes from numpy.savez (a zip file of the numpy arrays). ...\n",
        "    ├── ...           Being a zip-archive itself, this object can also be opened ...\n",
        "        ├── ...       as a zip-archive and browsed.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWAcc8RFRUoU"
      },
      "source": [
        "## Save and find "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tcQxzSCRUoV"
      },
      "source": [
        "# Create save dir\n",
        "save_dir = \"/tmp/gym/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model = PPO2('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n",
        "model.save(save_dir + \"/PPO2_tutorial\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGaMNz4HRUoX"
      },
      "source": [
        "!ls /tmp/gym/PPO2_tutorial*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYY3nQyyRUoa"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "archive = zipfile.ZipFile(\"/tmp/gym/PPO2_tutorial.zip\", 'r')\n",
        "for f in archive.filelist:\n",
        "  print(f.filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPKkkTvjRUo2"
      },
      "source": [
        "## Exporting saved models\n",
        "\n",
        "And finally some futher reading for those who want to export to tensorflowJS or Java.\n",
        "\n",
        "https://stable-baselines.readthedocs.io/en/master/guide/export.html"
      ]
    }
  ]
}